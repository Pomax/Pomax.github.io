I've written about digital photography before, but now that I own a high end mobile device with a decent camera and screen, it is constant source of frustration how terrible photography on $700 and above mobile devices is. Certainly, you could argue that they're phones, not cameras, but then you've not really been paying attention to what people actually *use* their phones for, where photography ranks considerably higher than calls.

I have a Nexus 6P, a high end Android device with, on paper, a rather good camera, and a very nice AMOLED screen. However, photographs taken with it, no matter whether that's done with the native camera app, CameraMX, Camera FV-5, or Open Camera, unless I treat the phone as "half the work", none of the pictures it take are worth anything if my intention is to share them with the rest of the world.

## Colour calibration - or, the lack thereof

If you care about your colours --and most photographers do, regardless of whether they're hobbyists or professionals-- you need colour calibration. On desktop this means colour-calibrating your monitor (either with an expensive calibrator, or with some open source and open hardware and personal effort), so that the colours it displays are the colours you shot (constrained by the absolute nonsense that is our acceptance of color gamuts, where even in 2016 we collectively don't seem to care that screens literally cannot display about half the colours the human eye sees in the real world). And, it means shooting a profiling target with your camera because calibration has to pass through every display chip: even if your monitor is calibrated, you still need to make sure that the color incorrectness of your camera can be compensated for as well (and oh boy can cameras be wrong)

So on an Android "phone" we have the same problem, but in a single device. We need to be able to calibrate the screen, because those fancy, super bright, super clear AMOLED screens actually get colour **super wrong**, and we need to be able to do shot calibration through the camera. 

Neither of those things are possible.

## Calibrating the screen

If you were smart enough to, after getting your Android device, immediately install an AOSP firmware instead, or even something like Cyanogen, you might think you have a leg up, because your enriched version of Android actually comes with colour calibration. Except it really doesn't, all it does is come with global colour *balancing*: you have the ability to control what the white point for your screen is, but that's it. If your screen's blues are a bit too red, and the reds are a bit too yellow, then no matter how much you fiddle with that white point, you're going to solve only one of those issues, while exacerbating the other.  Sure, it might make the AMOLED screen "less yellow", but that doesn't help us when it comes to making sure the colours we shoot with the device camera(s) match what they actually were (again, as constrained by the gamut we're forced to use).

Real calibration measures the response curves for each of the primaries that a display can generate, as well as the intensity responses curves for "greys" (with each primary firing at the same supposed intensity), and builds an ICC profile that lets the device tell the display how to change primary intensities independently of one another. Unlike a white point shift, where the display essentially just renders colours with an overlay filter, without looking at individual pixel values, true colour calibration involves rewriting colour values for every individual pixel.

And for reasons that I do not understand, $1000 imaging devices that you carry around in your pocket can't do that, whereas even a $25 graphics card in an ancient computer running Windows 95 will happily apply an ICC profile for you. *Surely* this is not a technical limitation at this point.

## Calibrating the camera

Camera calibration is more interesting because the only way to properly calibrate the output for a camera sensor is to take pictures of things with known colour values (still constrained to the gamut used for those colours values) and then compute the difference curves between "what the camera saw" and "what it should have reported seeing".

One problem here is that your eye has been fooling you for as long as you've been alive, and your brain rarely, if ever, notices that it is supremely good at lighting condition compensation. Differences in material properties mean that two different objects under two different lighting conditions will reflect different colours, *non-uniformly*: a matte white plaster and high gloss white plastic ball may reflect daylight in a way that lets us compute the difference between the white we think the objects should be, and the yellow that the objects actually are in sunlight. But if we change the lighting to halogen floodlights, the difference in absorption and reflection in the different materials means that the difference in colour between the two balls is *different* from the difference in colour between them in sunlight.

Color calibration is hard. 

This is why you'll see photographers carry around calibration targets: cards printed with specific colours on them that they'll periodically take pictures of during shoots so that during post processing they can ensure that for every shot under specific lighting conditions, there is a way to determine what the correction should look like. While display calibration is fairly static, scene calibration is a constant necessity.

Unfortunately, even though there are a number of commonly used scene calibration targets, neither Android nor IOS have ways to tell the phone itself to take a calibration with "calibration target X" so that the camera data that gets sent to a camera app is correct, which means that scene calibration on a mobile device is still the same as in traditional photography:  take a picture of a calibration target every now and then, and then "fix things in post".

Except on a mobile device, there **is** no post. There are no apps that perform batch corrects, there are no apps that can abstract a correction profile off of an IT8 or ColorChecker target, there are no apps that can load or embed ICC profiles into images. There is nothing, and so the only way to do half-decent photography with a $1000 device that is both camera and computer, is to have a **second** computer with a desktop operating system and real photo processing software.

## Why are you complaining, just use a real camera then

Fair enough. I have several real cameras, but I'm going to invoke one of my favourite quotes in recent years in a moment: my phone costs $750, that's an insane amount of money if all it could do was call and text, and so it can do a lot more: it has a high performance CPU and GPU, it has a high quality screen, and two (not just one) high quality fixed aperture camera sensors. And you're asking why I should expect to be able to do a thing that literally 100% of users of this kind of device do dozens of times a day, to the ability that the hardware allows, instead of "in the gimpiest broken way possible"? I'll tell you why, and here's that quote:

"Because this is bullshit, and we can fix this."