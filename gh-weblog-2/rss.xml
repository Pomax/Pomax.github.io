<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<atom:link href="http://pomax.github.io/gh-weblog-2/rss.xml" rel="self" type="application/rss+xml" />
<title>Pomax.github.io</title>
<description>My blog on github</description>
<link>http://pomax.github.io</link>
<lastBuildDate>Tue, 19 Mar 2019 16:04:08 GMT</lastBuildDate>
<pubDate>Tue, 19 Mar 2019 16:04:08 GMT</pubDate>
<ttl>1440</ttl>
<item>
<title>Using filters with your Lumix G Vario 7-14mm aspherical lens</title>
<description>&lt;p&gt;I own a &lt;a href="https://www.dpreview.com/reviews/panasonic-lumix-dc-gh5"&gt;Panasonic Lumix GH5&lt;/a&gt;, and it's pretty great, and I also own the &lt;a href="https://www.dpreview.com/reviews/panasonic-7-14-4-o20"&gt;Panasonic Lumix Vario G 7-14mm aspherical lens&lt;/a&gt; and it's &lt;em&gt;also&lt;/em&gt; pretty great, except for one problem: you can't put filters on it. It has a ridiculous integrated sun hood without any sort of screwthread so you can't screw even a clear filter to protect the lens not even from dropping, but from simple things like splash-drops hitting the lens glass and being hard to clean off.&lt;/p&gt;
&lt;p&gt;So I had a look around at filter adapters and they're either &lt;a href="https://www.amazon.com/Wonderpana-System-Olympus-7-14mm-Thirds/dp/B00AUK8XNG"&gt;very expensive&lt;/a&gt;, or &lt;a href="https://www.newsshooter.com/2014/05/05/dfocus-filter-adapter-for-panasonic-lumix-7-14-f4-0-designed-for-blackmagic-pocket-cinema-camera-users/"&gt;simply non-existent&lt;/a&gt;, so I've been trying to figure out something that works. I initially thought of using the lens cap that comes with the lens, trimming off the "cap" and then gluing a filter step ring to it so that I can screw in a filter, but as it turns out: you don't even need to do that.&lt;/p&gt;
&lt;p&gt;The sun hood &lt;em&gt;just barely&lt;/em&gt; fits in a 72mm step up ring, fitting pretty tight, and while 72-82mm is not big enough of a step (you'll see the edges of the ring at 7mm), 72-86mm works splendidly, so rather than buying a $100+ solution to mount unaffordable 105mm filters to your lens,  I would strongly recommend the following $14 solution instead:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;get a &lt;a href="https://www.amazon.com/gp/product/B009T1C1IU"&gt;72-86mm filter step-up ring&lt;/a&gt; for $6&lt;/li&gt;
&lt;li&gt;get an &lt;a href="https://www.amazon.ca/gp/product/B07HJ98MCG"&gt;86mm thin UV filter&lt;/a&gt; for $7&lt;/li&gt;
&lt;li&gt;you're done.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Well, almost: that sun hood isn't round, it's got some cutouts, so you probably want to still cut up your lens cap and use the resulting "barrel" to cover those holes so light can't get in from behind, reflect off the rear of the filter glass, and make its way into your shot. It's artsy, but probably not what you're going for.&lt;/p&gt;
&lt;h3 id="a-short-picture-tutorial"&gt;A short picture tutorial&lt;/h3&gt;
&lt;p&gt;Step 1: get your parts ready!&lt;/p&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/7-14mm filter/parts.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Step 2: place your step-up ring 86mm down, 72mm up, and press you lens all the way through it.&lt;/p&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/7-14mm filter/base.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Step 3: screw on your filter&lt;/p&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/7-14mm filter/filter.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Step 4: you're basically done.&lt;/p&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/7-14mm filter/done.jpg"&gt;&lt;/p&gt;
</description>
<category>Photography</category>
<category>Panasonic</category>
<category>Lumix</category>
<category>Vario</category>
<category>7mm</category>
<category>14mm</category>
<category>7-14mm</category>
<category>Lens</category>
<category>Filter</category>
<link>http://pomax.github.io/#gh-weblog-1553009656840</link>
<guid>http://pomax.github.io/#gh-weblog-1553009656840</guid>
<pubDate>Tue, 19 Mar 2019 15:34:16 GMT</pubDate>
</item>
<item>
<title> Adventures in HDMI land</title>
<description>&lt;p&gt;I like to cook, and I like to record the process, although I haven't been doing this nearly enough as I should. We moved, I got some new camera gear, before you know it, it's a year later. It happens.&lt;/p&gt;
&lt;p&gt;But, our new place has a fairly decent kitchen, and I'd like to record my prep and stove work, and so I wanted a setup where I could just stream when I was doing random cooking, while also recording when doing interesting dishes, and that poses a problem because the "prep stations" (two kitchen islands, one wood surfaced, one stainless/markble) are a good 3 meters away from the stove and range, and rolling a tripod with led panels and an HD recorder on it is fairly cumbersome. Especially since the prep stations need the lighting, whereas the stove/range already have plenty of decent light.&lt;/p&gt;
&lt;p&gt;So I figured I'd get a nice long HDMI cable, and only move the camera around. &lt;/p&gt;
&lt;p&gt;But this poses a problem.&lt;/p&gt;
&lt;h3 id="what-kind-of-hdmi-cable-do-we-need-"&gt;What kind of HDMI cable do we need?&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://www.hdmi.org/"&gt;HDMI&lt;/a&gt; is not one thing, there's a few standards, notably HDMI 1.4, HDMI 2.0, HDMI 2.0a, HDMI 2.0b, and HDMI 2.1, and their main differentiator is "bandwidth". That is: the amount of data per second you can push from one device to another device. "1080p", "3d", "dts audio", "ethernet", these are all fancy functions that only work if there's sufficient bandwidth.&lt;/p&gt;
&lt;p&gt;In fact, looking at a &lt;a href="https://en.wikipedia.org/wiki/HDMI#Main_specifications"&gt;comprehensive table of HDMI version&lt;/a&gt;, there's some interesting things going on: HDMI 1.* never makes it over 10 gigabits per second, whereas HDMI 2.0 can do 14 gbps, and HDMI 2.1 can do 42 gbps.&lt;/p&gt;
&lt;p&gt;But wait, there's more! Cables aren't perfect, and the longer the cable, the more signal degradation you get, to the point where individual frames (or even long sequences of frames) can get corrupted beyond the decoder's ability to fix, at which point you get black frames. Which is the opposite of what you want.&lt;/p&gt;
&lt;p&gt;So, what are some usable cable lengths? Well, that depends: what is the cable made of, and what method was it assembled in?&lt;/p&gt;
&lt;p&gt;Different cables use different materials and construction methods, and &lt;a href="https://en.wikipedia.org/wiki/Signal_reflection"&gt;signal reflection&lt;/a&gt; becomes a problem sooner or later depending on the quality of material and methods used.&lt;/p&gt;
&lt;p&gt;The highest quality copper "high speed" HDMI cables, capable of 18/14gbps, can theoretically reach 10 meters but in practice they almost never do. Of course, if you don't &lt;em&gt;need&lt;/em&gt; the maximum bandwidth, you can easily get a high quality 30 meter cable and be fine: 1080p from some video source to an HDMI projector, for instance, is absolutely no problem if you're using an HDMI 2.0 high speed cable. If you need a 4k signal, though, things get trickier.&lt;/p&gt;
&lt;h3 id="let-s-jump-into-chroma-subsampling-"&gt;Let's jump into Chroma subsampling!&lt;/h3&gt;
&lt;p&gt;Not only is 4k video four times as much data as 1080p video, but the higher resolution typically also comes with higher bit depth and more complex &lt;a href="https://en.wikipedia.org/wiki/Chroma_subsampling"&gt;"chroma subsampling"&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you're used to RGB pixels, where each pixel encodes its colour &lt;em&gt;and&lt;/em&gt; brightness at the same time, then chroma subsampling will probably be a new idea to you: instead of "full data", pixels are encoded in blocks of (typically) 2 rows of 4 pixels each, and while each pixel has its own brightness encoded, colour information gets applied to multiple pixels at a time, so you could have a set of eight pixels:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;p1 p2   p3 p4
p5 p6   p7 p8
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;with unique brightness information per pixel:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;b1 b2   b3 b4
b5 b6   b7 b8
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;but with colour information applied to blocks of multiple pixels:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; c1  (c1)   c2  (c2)
(c1) (c1)  (c2) (c2)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So in the above example: eight pixels, but the left four are "different shades of &lt;code&gt;c1&lt;/code&gt;" and the right four are "different shades of &lt;code&gt;c2&lt;/code&gt;". This reduces the bandwidth needed to transmit the data, at the expense of losing some of the information compared to true raw video footage.&lt;/p&gt;
&lt;p&gt;There are &lt;a href="https://en.wikipedia.org/wiki/Chroma_subsampling#Sampling_systems_and_ratios"&gt;a few standard schemes&lt;/a&gt; for chroma subsampling, with &lt;code&gt;4:4:4&lt;/code&gt; being "raw video": &lt;strong&gt;4&lt;/strong&gt; columns of 2, where the first row has &lt;strong&gt;4&lt;/strong&gt; unique values, and the second row has an &lt;em&gt;additional&lt;/em&gt; &lt;strong&gt;4&lt;/strong&gt; unique values, and every other &lt;code&gt;4:x:y&lt;/code&gt; value being subsampled.&lt;/p&gt;
&lt;p&gt;And of course, the more unique values that are supported by the subsampling scheme, the more data we have that needs to be transmitted, and the more bandwidth a cable needs to have to support the signal. And the faster signal reflection will become a problem.&lt;/p&gt;
&lt;h3 id="how-many-bits-are-you-using-"&gt;How many bits are you using?&lt;/h3&gt;
&lt;p&gt;Of course, chroma subsampling only gives you part of the information needed to figure out your bandwidth needs: we also need to know how many bits are used for the brightness and colour values. Much like digital photography, digital videography has a few options.&lt;/p&gt;
&lt;p&gt;The classic value for simple video is 8 bits. A simple &lt;code&gt;[0,255]&lt;/code&gt; range that works well for traditional computer graphics because regular monitors are (even today) typically 8 bit per colour channel. However, video doesn't get shot purely for typical computer monitors, and -as with digital photography- the fewer the bits, the less dynamic range, and dynamic range makes just as big a difference for video as it does for photography. So most video cameras can do 10 bits, and expensive video cameras can do 12, 14, or even 16 bits.&lt;/p&gt;
&lt;h3 id="oh-yeah-which-frame-rate-are-you-shooting-at-"&gt;Oh yeah, which frame rate are you shooting at?&lt;/h3&gt;
&lt;p&gt;Finally, the last thing that matters is how many frames of video data you're generating per second. Obviously, a 60 fps data stream is going need twice as much bandwidth as a 30 fps data stream. You might even shoot at 60 fps despite your final video using 24 fps, purely so you have more data available to work with during editing.&lt;/p&gt;
&lt;h3 id="back-to-hdmi-cables-"&gt;Back to HDMI cables!&lt;/h3&gt;
&lt;p&gt;And now we have everything we need to know to understand the limits of HDMI cables. The HDMI 2.0 standard defines a maximum bandwidth of 18.2 gigabits per second, so let's do some maths. A raw 4k frame constitutes 8.3 megapixels, which at 10 bit &lt;code&gt;4:4:4&lt;/code&gt; ("true", "raw", or "not subsampled"), would require 30 * 8.3 = 250 megabits per frame, which at 60 frames per second means we need 15 gigabits per second of bandwidth.&lt;/p&gt;
&lt;p&gt;And if we look back at the maximum data rate for HDMI 2.0, we don't get 15Gbps. We only get 14.4... so that could be a problem. In fact, video also usually has sound in addition to picture, so we now have negative bandwidth left to also add in audio. That really is a problem!&lt;/p&gt;
&lt;p&gt;Of course, most devices don't send out raw data, they apply at least &lt;em&gt;some&lt;/em&gt; kind of compression, and the same is true for HDMI video: rather than sending all the raw data, an HDMI video stream is typically encoded as &lt;a href="https://en.wikipedia.org/wiki/QuickTime_File_Format"&gt;&lt;code&gt;.mov&lt;/code&gt;&lt;/a&gt; or &lt;a href="https://en.wikipedia.org/wiki/MPEG-4_Part_14"&gt;&lt;code&gt;.mp4&lt;/code&gt;&lt;/a&gt;, with data compressed either as &lt;a href="https://en.wikipedia.org/wiki/Group_of_pictures"&gt;"long GOP"&lt;/a&gt;, blocks of frames (storing 1 real frame, and then a collection of "differences from one frame to the next", super handy for fixed camera shots with a fair amount of scene content that hardly changes over time), or as &lt;a href="https://en.wikipedia.org/wiki/Intra-frame_coding"&gt;"all-intra"&lt;/a&gt;, consisting of individual frames (useful when the camera moves and every single pixel will have a different value from frame to frame). So what we end up with is &lt;em&gt;quite a lot of parameters&lt;/em&gt; that all work together to determine whether or not your HDMI signal is going to make it from one end of the cable to the other end of the cable in one piece.&lt;/p&gt;
&lt;h3 id="back-to-the-kitchen-"&gt;Back to the kitchen!&lt;/h3&gt;
&lt;p&gt;So what &lt;em&gt;are&lt;/em&gt; my parameters? First off, my camera is a &lt;a href="https://www.panasonic.com/uk/consumer/cameras-camcorders/lumix-g-compact-system-cameras/dc-gh5l.specs.html"&gt;Panasonic Lumix GH5&lt;/a&gt;, capable of sending out 10 bit 4k video, with &lt;code&gt;4:2:2&lt;/code&gt; subsampling, at 60 fps, &lt;em&gt;as long as it doesn't also have to record that&lt;/em&gt;, which means it can either &lt;em&gt;send&lt;/em&gt; 10 bit &lt;code&gt;4:2:2&lt;/code&gt;, or &lt;em&gt;record&lt;/em&gt; 10 bit &lt;code&gt;4:2:0&lt;/code&gt;. It does not have the processing power to both read out a 10 bit &lt;code&gt;4:2:2&lt;/code&gt; video signal &lt;em&gt;and&lt;/em&gt; internally encode that to an SD card at the same time.&lt;/p&gt;
&lt;p&gt;(Could they have put a better cpu in the camera to make it able to do that anyway? Yep. Would it burn out because the camera would overheat in minutes? Absolutely. Consumer cameras aren't exactly designed for with good cpu cooling in mind. Fun fact: commercial digital video cameras super duper are)&lt;/p&gt;
&lt;p&gt;But that's okay, because SD cards are tiny, and 4k video file are huge, so instead of relying on the camera to save the video stream it's generating, I have an &lt;a href="https://www.atomos.com/ninja-inferno"&gt;Atomos Ninja Inferno&lt;/a&gt;, which is an external HDMI recorder capable of ingesting 4k, 10 bit &lt;code&gt;4:4:4&lt;/code&gt; video at 60fps, and write that to an SSD.&lt;/p&gt;
&lt;p&gt;Of course, the SSD needs to be &lt;em&gt;fast&lt;/em&gt; because while video isn't saved to file at 18gbps, it still needs a bloody fast drive to keep up with a high quality codec like ProRes, which uses about 1gbps for 10 bit &lt;code&gt;4:2:2&lt;/code&gt; "medium quality" video. And the SSD needs to be &lt;em&gt;capacious&lt;/em&gt; because at that data rate, a minute of footage takes up 8.8 gigabytes of space. &lt;/p&gt;
&lt;p&gt;That's right, a 1TB SSD will hold about two hours of ProRes-medium video, roughly the length of a single film these days. You thought we finally lived in an era where you could fit hundreds of films on a single 1 terabyte harddisk? Not when you're &lt;em&gt;recording&lt;/em&gt; those films!&lt;/p&gt;
&lt;p&gt;So, I have a camera spitting out 10 bit &lt;code&gt;4:2:2&lt;/code&gt; 60 fps 4k video, and a recorder capable of ingesting 10 bit &lt;code&gt;4:2:2&lt;/code&gt; 60 fps 4k. How long a cable can I put between those two?&lt;/p&gt;
&lt;h3 id="copper-cables"&gt;Copper cables&lt;/h3&gt;
&lt;p&gt;While HDMI 2.0-rated copper cables will happily do a 1080p signal over 30 feet or more, pushing the limit of what the cable can carry in terms of bandwidth severely limits how long a cable can get. Even a high quality cable will only do about 80 inches, 2 meters, of reliable signal. After that, frames start to drop, with intermittent signal deterioration past the point of being decodable, and while you might not notice that if your "recorder" is a TV, if it's a real recorder, those dropped frames are both unwanted, and very obvious. Imagine in your TV turned itself off every time a single frame dropped, and you can imagine the effect you get when frames drop for recording equipment.&lt;/p&gt;
&lt;p&gt;You can, however, improve this cable length by using a "powered HDMI cable", which is similar to a powered USB cable in that it takes the incoming signal, boosts it to something so strong that it can survive unattenuated for a longer distance, and then sends that signal instead. These cables have their own power supply (yep: cables with power supplies) and can certainly bridge lengths of 10 meters, but they're really bulky, and as such really only good for equipment that isn't going to be moved around much. Plus you'll need to have a power cable that is long enough, too.&lt;/p&gt;
&lt;p&gt;So: not super great for single-cameras setups.&lt;/p&gt;
&lt;p&gt;There are some alternatives, of course: there are &lt;a href="https://www.newegg.com/Product/Product.aspx?Item=9SIA6ZP8E36502"&gt;HDMI-to-ethernet&lt;/a&gt; and &lt;a href="https://www.blackmagicdesign.com/products/miniconverters"&gt;HDMI-to-SDI&lt;/a&gt; converters, where you run a (very) short high quality HDMI cable from your source to the converter, which then sends the signal on either over one or more &lt;a href="https://en.wikipedia.org/wiki/Category_6_cable"&gt;ethernet&lt;/a&gt; cables, or over &lt;a href="https://en.wikipedia.org/wiki/Serial_digital_interface"&gt;SDI cable&lt;/a&gt;, going into a converter back to HDMI at whatever destination you need to signal to get to. But those adapters also require power, and they're bulky, and so again, that's not going to work if you have a single camera that you need to move around.&lt;/p&gt;
&lt;p&gt;What does that leave?&lt;/p&gt;
&lt;h3 id="fiber-optic-cables"&gt;Fiber optic cables&lt;/h3&gt;
&lt;p&gt;If you were entertaining the idea of dropping $200 on ethernet converters anyway, you now also have the option to instead buy a $200 HDMI cable. Which sounds like an insane amount of money, but we're not talking about regular cables, we're talking about glass.&lt;/p&gt;
&lt;p&gt;Fiber optic HDMI cables have very-low-power converters in their plugs that convert electricity to optical signals, and then send light from one end of the cable to the other end, over distances that copper could never do, all the way up to 100 meters without needing external power.&lt;/p&gt;
&lt;p&gt;That's amazing! &lt;/p&gt;
&lt;p&gt;Obviously I bought one!&lt;/p&gt;
&lt;p&gt;...except fiber optic cables have a different problem: they're made of glass, and so they're fragile. Not "blow at them wrong and they break" fragile, but "transport them in a bouncy truck without enough padding and they will definitely stop working" fragile. And guess what happens when you order cables online? Which you'll have to do, because no local shop is going to carry $200 cables?&lt;/p&gt;
&lt;h3 id="adventures-in-hdmi-cable-ordering-"&gt;Adventures in HDMI cable ordering!&lt;/h3&gt;
&lt;p&gt;So yeah, I ordered one online, and by the time it got to my house it was thoroughly busted. If you like the flickering of a poorly wired lamp, imagine that, but then for video feeds. So I had to send it back and get a replacement.&lt;/p&gt;
&lt;p&gt;Which was also busted, flickering only marginally less.&lt;/p&gt;
&lt;p&gt;So I actually talked to the folks at Atomos who admitted that they, too, have to order a high number of them in the hopes of getting at least one that happened to work, so... that's what I did: I ended up ordering four cables, of which two were busted and two worked, so I kept one and sent the other three back.&lt;/p&gt;
&lt;p&gt;You might wonder how the people who make these cables even make any money, but remember that we're basically pushing the limit of what an HDMI cable can carry: all the cables that were busted for 10 bit &lt;code&gt;4:2:2&lt;/code&gt; 4k video at 60 fps worked &lt;em&gt;perfectly fine&lt;/em&gt; for 10 bit &lt;code&gt;4:2:2&lt;/code&gt; 4k video at 30 fps, as well as 8 bit &lt;code&gt;4:4:4&lt;/code&gt; 4k video at 60 fps, so people who order these cables may literally never discover their cable's damaged. Only folks who really need them for the upper limit of what HDMI 2 cables can carry will notice.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Hurray, that's me.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id="in-conclusion"&gt;In conclusion&lt;/h3&gt;
&lt;p&gt;HDMI is weird. On the one hand it's super useful and the spec is ever improving, but on the other cable manufacturers are not interested in making cables that actually support HDMI at max settings, and so if you need anything beyond a regular length cable, you're going to be in for a ride. &lt;/p&gt;
&lt;p&gt;HDMI 2.1 is out, it has more than twice the bandwidth of HDMI 2.0, at 48 gbps, but no one's making HDMI 2.1 cables yet. There's a Chinese company that claims to make them, but they're so incredibly fat (even though they're fiber optic) that you can't connect one to a camera without the weight of the cable off the ground either snapping the connector on a side connection, or just falling out from a bottom connection. &lt;/p&gt;
&lt;p&gt;So, if you need a long cable: order many of them. Ordering a single cable at a time is guaranteed to give you a broken cable (for your needs) and you'll just end up in an endless cycle of filing for refunds or replacements. Save up, order four (or more) in one go and then keep one that works and send the rest back either as defective, or as over-stocked. Maybe one day cable manufacturers will change their packaging and fiber optic cables will be safe to ship individually, but that day is not today, and I don't expect it to come any time soon.&lt;/p&gt;
&lt;h3 id="so-wait-which-did-you-get-"&gt;So wait, which did you get?&lt;/h3&gt;
&lt;p&gt;I got this one: the &lt;a href="https://www.amazon.ca/gp/product/B06XS8T2W4"&gt;Monoprice SlimRun AV HDR High Speed Cable for HDMI-Enabled Devices, 18Gbps, Fiber Optic, AOC, YUV &lt;code&gt;4:4:4&lt;/code&gt;, 50ft, Black&lt;/a&gt; and when it works, it's lovely. It's light, its stable, I can carry the camera all around the kitchen or mount it on the prep station tripod or the range camera boom in seconds. &lt;/p&gt;
&lt;p&gt;But again, if you're thinking of getting a cable like this:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;order many more than you need, and send the ones you can't use back.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;And then when you have a working cable: have fun!&lt;/p&gt;
</description>
<category>Photo</category>
<category>Video</category>
<category>HDMI</category>
<link>http://pomax.github.io/#gh-weblog-1544994322697</link>
<guid>http://pomax.github.io/#gh-weblog-1544994322697</guid>
<pubDate>Sun, 16 Dec 2018 21:05:22 GMT</pubDate>
</item>
<item>
<title> Listening to birds... from inside the house!</title>
<description>&lt;p&gt;We moved to a house on Vancouver Island not too long ago, to a fairly quiet area that's just outside a city, with lots of birdsong going on all day, every day. However, it's January and it's pretty cold out so while birdsong is lovely to listen to, opening a window isn't really an option.&lt;/p&gt;
&lt;p&gt;What is a couple of tech-minded people to do in a situation like this?&lt;/p&gt;
&lt;h2 id="-let-s-add-in-all-the-tech-"&gt;"Let's add in *all* the tech"&lt;/h2&gt;
&lt;p&gt;We ended up setting up an XLR conference microphone (the kind that needs 48V &lt;a href="https://en.wikipedia.org/wiki/Phantom_power"&gt;phantom power&lt;/a&gt; to work) outside, with a long (like, looong) cable running down the side, under the porch, into the house through a ventilation box, into the living room, and into a USB XLR mixer (a Behringer &lt;a href="https://www.long-mcquade.com/67988/Pro_Audio_Recording/Audio_Interfaces/Behringer/U-Phoria_UMC404HD_Audiophile_4x4_24-Bit_192_kHz_USB_Audio_MIDI_Interface.htm"&gt;U-Phoria UMC404HD&lt;/a&gt;), and then using DDMF's &lt;a href="http://www.virtualaudiostream.com"&gt;Virtual Audio Stream&lt;/a&gt; to set up a digital audio filter chain from the microphone input to the speakers, using their "Effect Rack" utility.&lt;/p&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/bird-vst/the-setup.jpg" alt=""&gt;&lt;/p&gt;
&lt;p&gt;Now we can listen to birds (oh my god, so many birds) while working inside! (at least until the weather improves and we can open the windows like normal people would =)&lt;/p&gt;
&lt;h2 id="the-filter-chain"&gt;The filter chain&lt;/h2&gt;
&lt;p&gt;The following image shows the filter chain we're using in Effect Rack:&lt;/p&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/bird-vst/ddmf-filter-chain.png" alt=""&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;First, a high-pass filter to cut out definitely-not-related-to-birdsong low frequencies.&lt;/li&gt;
&lt;li&gt;Then, an automatic noise reduction filter. This does a relative good job are removing unwanted noise, but not quite enough.&lt;/li&gt;
&lt;li&gt;That's followed by a multi-band compressor, which is used to further cut out any frequencies after noise reduction that are definitely not in the range of bird song.&lt;/li&gt;
&lt;li&gt;This is followed by a second single-pass compressor that aggressively dampens any sound that's louder than nearby birdsong, such as cars driving by close to the mic, or someone using a chainsaw the next field over.&lt;/li&gt;
&lt;li&gt;Finally, there's an equaliser that's being used as an aggressive high frequency cut off, throwing away everything that's above 10KHz&lt;/li&gt;
&lt;li&gt;There's also a little spectrum analyzer, which helps to tune the previous filters, since it's pretty obvious which frequencies bird song is happening at.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You may have noticed that only the right channel seems to be hooked up, except all the way at the end: this is because the microphone only generates a single channel, it's not a stereo microphone, and so all the processing really only needs to happen to the one channel. However, because it's nice to have the audio come out of both speakers, all the way at the end "all channels" are sent to "all speakers")&lt;/p&gt;
&lt;p&gt;So, in case you're curious, let's look at each filter in detail.&lt;/p&gt;
&lt;h2 id="cutting-out-the-low-frequencies-"&gt;Cutting out the low frequencies.&lt;/h2&gt;
&lt;p&gt;The initial high pass filter (a signal processing term for "thing that lets frequencies above X through, while setting the volume for frequencies below X to zero) is an instance of &lt;a href="https://www.meldaproduction.com/MBandPass"&gt;Melda Production's MBandPass&lt;/a&gt; filter, set pretty aggressively. &lt;/p&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/bird-vst/bandpass.png" alt=""&gt;&lt;/p&gt;
&lt;p&gt;Everything under 224Hz gets thrown away, using a 12dB knee, with insanely low quantization. This actually does a great job at removing most car noise without affecting the rest of the audio too much. &lt;/p&gt;
&lt;h2 id="removing-line-and-signal-noise-"&gt;Removing line and signal noise.&lt;/h2&gt;
&lt;p&gt;To remove general noise, both because the outside world is noisy, and because there's a long cable on the mic, I'm using &lt;a href="https://acondigital.com/products/restoration-suite"&gt;Acon's noise reduction filter&lt;/a&gt; with a 1.83 second sliding window, meaning that it looks at 1.82 seconds of audio that's come by, tries to find the "noise profile" in that, and then applies it to the current signal.&lt;/p&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/bird-vst/noise-reduction.png" alt=""&gt;&lt;/p&gt;
&lt;p&gt;While this is almost guaranteed to always be "wrong", it's long enough to get a decent noise profile, while being short enough to still remove &lt;em&gt;most&lt;/em&gt; noise from the audio signal.  &lt;/p&gt;
&lt;h2 id="cutting-out-these-are-not-birds-sounds-"&gt;Cutting out "these are not birds!" sounds.&lt;/h2&gt;
&lt;p&gt;Birdsong is doesn't use a lot of low frequencies, so in order to further weed out any frequencies that we don't actually want to be listening to, I use &lt;a href="https://www.image-line.com/plugins/Effects/Maximus"&gt;Image-Line's "Maximus" multi-band compressor&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A compressor is a thing that can take an incoming audio signal, look at the volume-per-frequency range, and then change that range. The most common use is to reduce the volume on really loud noises, so that if you talk into a mic it does nothing, but if you yell into the mic, the volume doesn't suddenly blow out your speakers - that's not how it's being used here.&lt;/p&gt;
&lt;p&gt;Instead, we're applying three different compressor settings to each of the low, mid, and high bands in the audio signal.   &lt;/p&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/bird-vst/low-compressor.png" alt=""&gt;&lt;/p&gt;
&lt;p&gt;In the low band (0Hz-200Hz), we change the volume range [0,max] to the volume range [0,0]. We literally just throw &lt;em&gt;all&lt;/em&gt; of it away. The preceding high pass filter should have already taken care of that, but it doesn't hurt to make sure low frequencies stay gone.&lt;/p&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/bird-vst/mid-compressor.png" alt=""&gt;&lt;/p&gt;
&lt;p&gt;In the mid band (200Hz-3KHz), we change the volume mapping so that anything that's loud, stays loud, but anything that's soft is pretty much scaled waaaay back down. To zero, in fact, for most volumes. Or somewhere progressively closer to the original level on the high end of the spectrum's "mid" band. &lt;/p&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/bird-vst/high-compressor.png" alt=""&gt;&lt;/p&gt;
&lt;p&gt;In the high frequencies band (3KHz and up), which is where most bird song is found, we don't really do much: this is a typical compressor function that just makes sure that things that are too loud to be reasonable get scaled down in volume. In case a bird yells right next to the microphone, mostly.&lt;/p&gt;
&lt;p&gt;I'm looking at you, Stellar's Jays... =_=&lt;/p&gt;
&lt;h2 id="making-sure-a-sudden-car-or-chainsaw-doesn-t-give-us-a-heart-attack-"&gt;Making sure a sudden car or chainsaw doesn't give us a heart attack.&lt;/h2&gt;
&lt;p&gt;Even with all that compressor work, there's still the chance that cars, chainsaws, or people yelling end up generating very loud noises in the same range that bird song occurs in, so we add one more "overall" compressor. &lt;/p&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/bird-vst/second-compressor.png" alt=""&gt;&lt;/p&gt;
&lt;p&gt;This is &lt;a href="https://www.meldaproduction.com/MCompressor"&gt;Melda Production's MCompressor&lt;/a&gt;, and its job is to simply take the overall signal, and dampen it when it's too loud.&lt;/p&gt;
&lt;h2 id="cleaning-up-any-compound-distortion-at-the-high-end-"&gt;Cleaning up any compound distortion at the high end.&lt;/h2&gt;
&lt;p&gt;You might be wondering about whether all those filters one after another don't generate some kind of artificial noise, and the answer is: oh no, they absolutely do. So to combat the noise that is most audible, I use an equaliser filter (&lt;a href="https://www.image-line.com/support/FLHelp/html/plugins/EQUO.htm"&gt;Image Line's EQUO&lt;/a&gt;) to pretty much throw away anything about 10KHz, with a gentle volume reduction starting at 2.5KHz, which does a nice job at throwing away the high frequency noise that previous filter chain generated.&lt;/p&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/bird-vst/eq-cutoff.png" alt=""&gt;&lt;/p&gt;
&lt;p&gt;You might think "well why not use a low pass filter?" as that's essentially what this does, and the reason is mostly because I wanted a little more more control over the knee and ramp coming down from 2.5KHz which is exactly what an equalizer lets you do. &lt;/p&gt;
&lt;h2 id="and-that-s-it-all-that-s-left-is-to-enjoy-"&gt;And that's it. All that's left is to enjoy.&lt;/h2&gt;
&lt;p&gt;The last filter is &lt;a href="https://www.meldaproduction.com/MAnalyzer"&gt;Melda Production's "MAnalyzer"&lt;/a&gt;, which doesn't actually do anything to the audio signal (it just lets it pass through unchanged), but instead simply shows what's happening in terms of which frequencies are playing how loud on a moment-to-moment basis. This is very useful to see where unwanted frequencies are still coming through, to optimize any of the previous filters, plus... it looks pretty!&lt;/p&gt;
&lt;h2 id="so-what-does-it-sound-like-"&gt;So what does it sound like?&lt;/h2&gt;
&lt;p&gt;Of course, all this talk about bird song leads to the inevitable "okay, but... what does it sound like?". So here are two clips. One is the "raw audio" as simply captured by the mic outside straight from the XLR mixer, on a mildly rainy day - there's some birds twittering, a car drives by, it's mostly unimpressive but it's contrasted against the same audio after it's been run through the filter chain, representing what we hear in our home office through the speakers.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://clyp.it/ksmbapw1?token=5fca3db27da4d26cfea827dbf5295608"&gt;the raw audio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://clyp.it/2hj4kmfc?token=b535e17e75cc3f535b75927cec1203d1"&gt;the filtered audio&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I'll leave you to decide which sounds better, and while you do, I'll be going back to enjoying our most excellent bird song!&lt;/p&gt;
&lt;p&gt;(And if you have cleanup recommendations, leave a comment! Tips and tricks are always welcome)&lt;/p&gt;
</description>
<category>Audio</category>
<category>VST</category>
<category>Birds</category>
<link>http://pomax.github.io/#gh-weblog-1517436884711</link>
<guid>http://pomax.github.io/#gh-weblog-1517436884711</guid>
<pubDate>Wed, 31 Jan 2018 22:14:44 GMT</pubDate>
</item>
<item>
<title> Creating VST/AU/etc audio plugins in 2017</title>
<description>&lt;p&gt;Probably the best tutorial on getting started writing a VST plugins (whether you want to make an instrument or a filter) is Martin Finke's &lt;a href="http://www.martin-finke.de/blog/tags/making_audio_plugins.html"&gt;Making Audio Plugins&lt;/a&gt;. It covers going from "zero to hero" but it has one downside: it was written in 2012, and using equivalent tools from 2017 leads to some issues when trying to get to a point where you can actually compile your plugin. Mind you, they're not big issues, but they're "big enough to frustrate people who would otherwise dive right in, losing them as they walk away to do something else instead".&lt;/p&gt;
&lt;p&gt;So, what can we do?&lt;/p&gt;
&lt;p&gt;First off, it's 2017 (coming up to 2018) and I'm going to assume you're using Windows mostly because that's what I'm using (because several audio production tools I use only exist for Windows. I also have a mac, yes I own Ableton, no I'm not covering using XCode in 2017 here. But if you want to do that work for me, I will be more than happy to add your findings to this blog post! (hit me up &lt;a href="https://github.com/pomax/pomax.github.io/issues"&gt;here&lt;/a&gt; to discuss that!).&lt;/p&gt;
&lt;h2 id="getting-the-right-tools"&gt;Getting the right tools&lt;/h2&gt;
&lt;p&gt;You're probably on Windows 10, so we want to install &lt;a href="https://www.visualstudio.com/downloads/"&gt;Visual Studio 2017 Community Edition&lt;/a&gt; with all the SDKs checked during the installation. That's pretty important. We need those SDKs.&lt;/p&gt;
&lt;h2 id="following-the-instructions"&gt;Following the instructions&lt;/h2&gt;
&lt;p&gt;While "Making Audio Plugins &lt;a href="http://www.martin-finke.de/blog/articles/audio-plugins-002-setting-up-wdl-ol/"&gt;part 2&lt;/a&gt;" covers most of it, once you've used python to create the directory with all the files for your first plugin, and you're loading the &lt;code&gt;.sln&lt;/code&gt; file in Visual C++, you're going to hit warnings and errors.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Don't worry, it's okay&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Accept the suggestion by VC++ to upgrade the project to the modern SDKs (which it will fail at, but again, that's okay), and then instead of trying to compile &lt;code&gt;MyFirstPlugin-app&lt;/code&gt;, first make sure the project and Visual C++ know that we're working on Windows 10.&lt;/p&gt;
&lt;h3 id="update-the-platform"&gt;Update the platform&lt;/h3&gt;
&lt;p&gt;Go to &lt;code&gt;Project&lt;/code&gt; and then &lt;code&gt;Properties&lt;/code&gt;, and in the "General" settings, change the platform tools from "Visual Studio 2017 - Windows XP (v...)" to just "Visual Studio 2017 (v...)", so that we're not trying to compile against anything Windows-XP-related.&lt;/p&gt;
&lt;p&gt;If you don't do this, you'll see errors like the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Warning    MSB8003
Could not find WindowsSdkDir_71A variable from the registry.
TargetFrameworkVersion or PlatformToolset may be set to an invalid version number.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Which, aside from being horribly poor grammar, is also a red herring: what this really mean is "the project is using an SDK that cannot be found", which is true: we're on Windows 10, we're using SDK version 141 (or perhaps by the time you read this, even higher), rather than version 71.&lt;/p&gt;
&lt;h3 id="update-your-include-path-probably"&gt;Update your include path, probably&lt;/h3&gt;
&lt;p&gt;If, after updating the platform tools, you try to compile and you get &lt;code&gt;missing windows.h&lt;/code&gt; and/or &lt;code&gt;missing winapifamily.h&lt;/code&gt; errors, then we need to manually intervene because Visual Studio did not pick the correct paths for including common headers.&lt;/p&gt;
&lt;p&gt;In the project properties, go to the "VC++ Directories" section, because we'll need to add two directories to the "Includes" path, which is probably already filled in as &lt;code&gt;$(VC_IncludePath);$(WindowsSDK_IncludePath);&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;We'll need to add the &lt;code&gt;\mu&lt;/code&gt; and &lt;code&gt;\shared&lt;/code&gt; directories for the Windows SDK that we're working with, which as of the date of this post is the Windows 10 Fall Creator Edition SDK version 10.0.16299.0, found (by default - if you changed your installation path then you should know what to do here) in:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;C:\Program Files (x86)\Windows Kits\10\Include\10.0.16299.0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Given that location, we will need to add the following two paths to the include path in the project properties include path:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;C:\Program Files (x86)\Windows Kits\10\Include\10.0.16299.0\mu;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;C:\Program Files (x86)\Windows Kits\10\Include\10.0.16299.0\shared;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So we end up with the Include path string "$(VC_IncludePath);$(WindowsSDK_IncludePath);C:\Program Files (x86)\Windows Kits\10\Include\10.0.16299.0\mu;C:\Program Files (x86)\Windows Kits\10\Include\10.0.16299.0\shared;".&lt;/p&gt;
&lt;h3 id="retargeting-the-project-possibly"&gt;Retargeting the project, possibly&lt;/h3&gt;
&lt;p&gt;Finally, it's possible that trying to compile after these two steps still doesn't work. In that case, right-click on &lt;code&gt;MyFirstPlugin-app&lt;/code&gt; and choose "retarget projects". This will pop up a dialog that lets you pick the SDK version. Make sure to pick the &lt;code&gt;10.0.....&lt;/code&gt; version that matches what we used for the Platform Tools in the project properties, and accept that change.&lt;/p&gt;
&lt;h2 id="you-should-be-good-to-go-"&gt;You should be good to go!&lt;/h2&gt;
&lt;p&gt;Alright, now we can get back to that awesome tutorial and make some audio plugins. Select &lt;code&gt;MyFirstPlugin-app&lt;/code&gt;, hit F5 to start a compile, and "things should just work(tm)".&lt;/p&gt;
</description>
<category>Audio</category>
<category>VST</category>
<category>Programming</category>
<category>VC++</category>
<category>2017</category>
<link>http://pomax.github.io/#gh-weblog-1512169175433</link>
<guid>http://pomax.github.io/#gh-weblog-1512169175433</guid>
<pubDate>Fri, 01 Dec 2017 22:59:35 GMT</pubDate>
</item>
<item>
<title>A beer riddle</title>
<description>&lt;p&gt;Cold beer I promise, if but in name; pour me one for the difference to remain. Lakes of beer I pass in strain - what am I?&lt;/p&gt;
&lt;p&gt;&lt;span style="background: black; color: black"&gt;
My name spells cold yet warm I work, my job to offer beer a fork: swim along or cloud the flow. It is my job to tell it: "no". What am I?
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="background: black; color: black"&gt;
Once in water, I now swim here, holding that which muddles beer. Separate it from what is dear, thrown away once beer is clear. What am I?
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And the answer:&lt;/p&gt;
&lt;p&gt;&lt;span style="background: black; color: black"&gt;
From fish we get a jellied web, that filters the last bits of drab, from finished beers sent off for casking: "&lt;a style="color: black" href="https://en.wikipedia.org/wiki/Isinglass"&gt;isinglass&lt;/a&gt;" the name of asking.
&lt;/span&gt;&lt;/p&gt;
</description>

<link>http://pomax.github.io/#gh-weblog-1490481349209</link>
<guid>http://pomax.github.io/#gh-weblog-1490481349209</guid>
<pubDate>Sat, 25 Mar 2017 22:35:49 GMT</pubDate>
</item>
<item>
<title> Localization is hard.</title>
<description>&lt;p&gt;I know, that seems like an obvious statement, but I want to dig a little deeper because just because you have a "web thing" that people might want to see translated or even fully localized, that doesn't mean that they can, or even if they can, that it's easy for them to do so.&lt;/p&gt;
&lt;p&gt;As such, this is a story of a project that was blessed with two independent users wanting to translate content to Chinese and Japanese, and a code base that was &lt;em&gt;absolutely not ready for that&lt;/em&gt;. As of this blog post, it most definitely is, and that's been a few weeks worth of journey.&lt;/p&gt;
&lt;p&gt;Let me take you through the travel log.&lt;/p&gt;
&lt;h2 id="once-upon-a-time-there-was-a-primer-on-b-zier-curves-https-pomax-github-io-bezierinfo-"&gt;Once upon a time there was a &lt;a href="https://pomax.github.io/bezierinfo"&gt;Primer on Bézier curves&lt;/a&gt;.&lt;/h2&gt;
&lt;p&gt;Or rather, there was a &lt;a href="http://processingjs.org"&gt;Processing.js&lt;/a&gt; based exploration of the basics of Bézier curves because I was working on font generation and wanted to understand the intricacies of Bézier curves for the purposes of drawing curve outlines. There weren't any really good resources on the topic, and because the best way to learn something is to teach it, I figured I'd write my own explanation, put it online, and learn how they worked that way.&lt;/p&gt;
&lt;p&gt;As I learned more and more I kept updating that one short page until it had became a rather long page, and once it started to show up fairly high in Google results for "bezier curves" (typos included because who has the time to write "é" when you're not on a French keyboard) I started taking "making sure it was sort of usable" more seriously.&lt;/p&gt;
&lt;p&gt;Jumping from 2011 to 2017, the page underwent a fair number of changes, and I thought I had a pretty good setup until a few weeks ago:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Primer itself is a &lt;a href="https://facebook.github.io/react"&gt;React&lt;/a&gt;-managed single page... err... page. It's just a page, not an app.&lt;/li&gt;
&lt;li&gt;Each section is a &lt;a href="https://github.com/Pomax/BezierInfo-2/tree/4c3be7105161d69774e622432654a5dbc62bae96/components/sections"&gt;separate JSX file&lt;/a&gt; wrapping both the JS functionality and the web content, with the &lt;code&gt;render()&lt;/code&gt; function simply housing a big block of almost-HTML code with virtually no templating, so it reads like normal webpages (for the most part).&lt;/li&gt;
&lt;li&gt;the article as a whole loads all the sections, &lt;a href="https://github.com/Pomax/BezierInfo-2/blob/4c3be7105161d69774e622432654a5dbc62bae96/components/Navigation.jsx"&gt;mines them for title data, uses that to build a ToC&lt;/a&gt;, and then just renders the entire article.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It also has clever code for dealing with "I just want to write about maths, not program about maths", so sections can contain code that isn't legal JSX in the slightest, but that's okay: babel loaders do some preprocessing and when the code actually gets to the "convert react to plain JS" step, everything is perfectly fine:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bundling uses &lt;a href="https://webpack.github.io"&gt;Webpack&lt;/a&gt;, with &lt;a href="http://babeljs.io/"&gt;babel&lt;/a&gt; for converting JSX and ES6+ code to plain old ES5&lt;/li&gt;
&lt;li&gt;a number of custom webpack loaders are used to turn "my code" into "actually spec-compliant JSX" (insofar as there is a spec, of course):&lt;ul&gt;
&lt;li&gt;a &lt;a href="https://github.com/Pomax/BezierInfo-2/blob/4c3be7105161d69774e622432654a5dbc62bae96/lib/p-loader.js"&gt;&lt;code&gt;p-loader&lt;/code&gt;&lt;/a&gt; which replaces &lt;code&gt;{&lt;/code&gt; and &lt;code&gt;}&lt;/code&gt; inside paragraphs with JSX-safified versions instead, because to JSX those are templating instructions.   &lt;/li&gt;
&lt;li&gt;a &lt;a href="https://github.com/Pomax/BezierInfo-2/blob/4c3be7105161d69774e622432654a5dbc62bae96/lib/pre-loader.js"&gt;&lt;code&gt;pre-loader&lt;/code&gt;&lt;/a&gt; which replaces "html sensitive" characters in &lt;code&gt;&amp;lt;pre&amp;gt;&lt;/code&gt; blocks with safe equivalents, so that I can write &lt;code&gt;i &amp;lt; 0&lt;/code&gt; rather than having to bother with remembering to use &lt;code&gt;i &amp;amp;lt; 0&lt;/code&gt;. Which is good: having to remember HTML entities is dumb.&lt;/li&gt;
&lt;li&gt;a &lt;a href="https://github.com/Pomax/BezierInfo-2/blob/4c3be7105161d69774e622432654a5dbc62bae96/lib/latex-loader.js"&gt;&lt;code&gt;latex-loader&lt;/code&gt;&lt;/a&gt; which does some "genuinely cool shit™"&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;There is a &lt;code&gt;&amp;lt;SectionHeader&amp;gt;&lt;/code&gt; component that ensure that each section ends up providing its own fragment identifier for easy &lt;code&gt;index.html#somesection&lt;/code&gt; navigation and bookmarking.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Particularly the LaTeX loader is a thing I really like: &lt;a href="http://pomax.github.io/1451617530567/react-with-latex-without-needing-client-side-mathjax"&gt;I wrote about this before&lt;/a&gt;, but for the longest time the on-page LaTeX would simply be parsed client-side using &lt;a href="https://mathjax.org"&gt;MathJax&lt;/a&gt; but that made the article load really slowly because I use a lot of maths. Like... &lt;strong&gt;a lot&lt;/strong&gt;. People had to basically wait for 150+ blocks of LaTeX to be typeset by their browser, slowing down the page load, making the page unresponsive, and basically just being super annoying. Worth it for what you got in return, but still annoying.&lt;/p&gt;
&lt;p&gt;So, eventually, I wrote the latex-loader whose specialized role was to ingest a JSX file, extract all LaTeX code blocks, convert them &lt;em&gt;offline during the build&lt;/em&gt; by running them through MathJax (using &lt;a href="https://www.npmjs.com/package/mathjax-node"&gt;mathjax-node&lt;/a&gt;), saving the resulting SVG to file, and then generating a &lt;em&gt;new&lt;/em&gt; JSX file in which each LaTeX block got replaced with &lt;code&gt;&amp;lt;img src="latexfile.svg" width="..." height="..."/&amp;gt;&lt;/code&gt; to effect two things: first, excellent load experience for users because their browser wasn't locking up anymore, and second, the page content wouldn't move around anymore because while client-side MathJax means there was no way to tell how much vertical space a conversion result would take up, having actual SVG files means you can just quickly check what their size is going to be and bind an image &lt;code&gt;width&lt;/code&gt; and &lt;code&gt;height&lt;/code&gt; value so that the DOM already knows how much space it needs even before an image is loaded.&lt;/p&gt;
&lt;p&gt;All in all, it was &lt;em&gt;pretty&lt;/em&gt; sweet!&lt;/p&gt;
&lt;h2 id="and-then-contributors-show-up-"&gt;And then contributors show up.&lt;/h2&gt;
&lt;p&gt;I've been working on the Primer pretty much on my own since 2011, with some help from people in terms of advice or suggestions, but never in the sense that they took the code, changes something radically, and then filed a PR and discussed the changes to get them landed.&lt;/p&gt;
&lt;p&gt;In February of 2017, that changed. In the span of a week two people contacted me because they wanted to translate the article to their own language. Specifically: Chinese and Japanese. And this is where, if you're an English content creator, things get interesting because that's not just translation: that's localization. While of course translation is involved, things like "how maths statement are organized" also changes, and probably some other things you hadn't even thought of.&lt;/p&gt;
&lt;p&gt;Naturally, I really wanted to take these folks up on their offer: getting the Primer translated so that the content is more meaningful for Chinese and Japanese audiences is huge! But my code base was written by one person, for one person, for one language, without any affordances to the possibility of eventual translation. so I did the only thing I could think of:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I told them I was super interested in having them help out, but would need a little time to make sure they could work on the code&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This is where you lose contributors, by the way, but it's still the correct way to deal with people showing up wanting to help doing a thing you had never spent any time thinking of at all: when the world shows up at your doorstep asking to help you, you make it some tea, express your deepest gratitude and then while they're hopwfully distracted by the cookies you work your ass off to make sure it doesn't go "well I should go again". You make it want to stay.&lt;/p&gt;
&lt;h2 id="code-base-changes-let-s-talk-about-failures-"&gt;Code base changes: let's talk about failures.&lt;/h2&gt;
&lt;p&gt;I've worked with localized code bases before and there are some nice solutions out there for websites and apps. I figured I'd see if &lt;a href="https://www.transifex.com"&gt;Transifex&lt;/a&gt; and &lt;a href="https://pontoon.mozilla.org"&gt;Mozilla's Pontoon&lt;/a&gt; could be used, because that's what I've used in the past, but this is where traditional localization solutions break down.&lt;/p&gt;
&lt;p&gt;Transifex is a translation service that lets you define your project in terms of key-value mappings, where you use the keys in your own content, and translators can create translations of the associated values to whatever language you want to have supported. You then download your latest translations from transifex and apply the key/value mappings to your content to achieve a translated web page, app, UI, etc. Now, while this works quite really well for web apps, and really well for general UI,  things get tricky for content like articles. In articles, where the content is structured in paragraphs and the ordering matters for the tone of the text, asking localizers to translate paragraphs or even single sentences fully detached from what comes before or after is almost guaranteed to give weird translations.&lt;/p&gt;
&lt;p&gt;To alleviate that problem, Mozilla's Pontoon project offers a sort of "on-page translation" localization system, where you load the website and pontoon UI at the same time, and translators can "double click and translate" text on the page. While that sounds really nice, it turns out that setting it up for "not mozilla sites" is a little bit tricky: you need to run your own copy of pontoon, and a lot of the code assumes it's running on Mozilla infrastructure. However, once you do get that it work, you still have the problem that transifex has: your localizers might have an easier time, but as an author you're still stuck being unable to write text yourself without then having to convert it to an unreadable "mess" of &lt;code&gt;getText('section1-paragraph1')&lt;/code&gt; in your code, so you have no idea what you wrote. You can try to get abstract keys from values (say, by hashing) but then changing even one letter in a paragraph invalidates every translation, which is typically &lt;em&gt;not&lt;/em&gt; what you want. A little desync is fine for webcontent. Corrections that trickle through are infinitely preferable to "keeping either all sites on the old content, or breaking all translations for a hopefully short while as translators catch up with changes".&lt;/p&gt;
&lt;p&gt;So what do you do?&lt;/p&gt;
&lt;p&gt;The problems with the methodology of these translation/localization systems outweight the benefits they brought to my project, as they either didn't work for me as author, or didn't work for the nature of the content, so the traditional solutions for adding localization to a site were out. Not because they're unsuitable as localization solutions, but because for this project, they introduced more problems than they solved.&lt;/p&gt;
&lt;h2 id="code-base-changes-that-work-markdown-"&gt;Code base changes that work: Markdown.&lt;/h2&gt;
&lt;p&gt;And so after about a week of trying to make localization "easy" with string based localization services, instead I figured I'd roll my own solution because I was keeping people waiting for a week at this point and they had every reason to just go "well this Pomax guy's clearly not interested in getting help, time to look for something else to help out on". &lt;/p&gt;
&lt;p&gt;This is, by the way, your single biggest challenge in Open Source: if you thought it was hard to find contributors, it's ten times harder to make sure &lt;strong&gt;you&lt;/strong&gt; respond in a timely fashion and make sure there is something for them to get started on. Don't have anything? Take a paid-time-off or vacation day and &lt;em&gt;make sure there are things for them to start on&lt;/em&gt;. Or lose your contributor. If you leave a contributor hanging for more than a day, your project isn't really worth helping out on.&lt;/p&gt;
&lt;p&gt;So I had to make this work, and the solution to the problem was one that basically comes to us from the traditional translation world: detach the content from the code, and translate the content as full length pieces. This is programming, we can make "integrating disparate pieces of code and content" work in at least seventeen different ways, so it's entirely possible to offer authors and translators alike a base case where "here is the section as natural, flowing text, with embedded LaTeX where necessary and code blocks to illustrate how to program stuff. Just translate that". And so that's what I set out to achieve.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I decided on a content format: sections would be an &lt;code&gt;index.js&lt;/code&gt; for the JSX code, and a &lt;code&gt;content.en-GB.md&lt;/code&gt; for my own English content.&lt;/li&gt;
&lt;li&gt;content would be pulled back into the JSX by... wait...&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How do you pull markdown content into a JSX file? Unlike &lt;code&gt;js&lt;/code&gt; or &lt;code&gt;jsx&lt;/code&gt; or &lt;code&gt;json&lt;/code&gt;, markdown content can't just be imported in JavaScript or even in Node.js... It's a problem, but really solving this problem before getting back to my contributors would prevent translations from happening, and so &lt;em&gt;I stepped away from the problem and told them I had something they could work with now&lt;/em&gt;. I had a solution &lt;em&gt;for them&lt;/em&gt;, and that came first.&lt;/p&gt;
&lt;p&gt;Yes, it's nice to solve these things as they pop up, but the most important part is still to make sure contributors can start doing what they wanted to do to help, and once I figured out how to at least split up the JSX into "JSX for the skeleton" and "Markdown for the content", I had a solution that unblocked my contributors so that &lt;em&gt;they&lt;/em&gt; could now at least get started translating and making progress, even if the system for rebuilding the content wasn't done yet and I wouldn't be able to immediately deploy whatever PR they were going to throw my way.&lt;/p&gt;
&lt;p&gt;Their needs came first.&lt;/p&gt;
&lt;p&gt;Once I figured out how to at least split up the content as JSX and Markdown, I sat down and split up the preface and first three sections for the primer, creating a few &lt;code&gt;content.en-GB.md&lt;/code&gt; that set the tone for what the markdown would look like, and then I told the hopefully-still-willing-to-be-contributors that if they were still interested in helping out they could now start on these files. All they had to do was copy it to &lt;code&gt;content.zh-CN.md&lt;/code&gt; or &lt;code&gt;content.ja-JP.md&lt;/code&gt; and then modify that as best they knew how to.&lt;/p&gt;
&lt;p&gt;And while they were doing that, I'd have some time to implement getting the Markdown loaded back into the JSX files to generate a site that, for visitors, was identical to the monolithic English one.    &lt;/p&gt;
&lt;p&gt;The take-away here is mostly that perfect solutions aren't necessary: if you're balancing priorities &lt;strong&gt;unblock your contributors before you unblock yourself&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="reintegrating-sections-based-on-jsx-and-markdown"&gt;Reintegrating sections based on JSX and Markdown&lt;/h2&gt;
&lt;p&gt;While the contributors were working on their translations, I got back to work integrating Markdown into the JSX, and after a bit of thinking the solution to how to achieve that integration was remarkably simple: &lt;em&gt;you don't&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;I know, that sounds a bit silly, but it's not silly as you might initially think; I solved this problem using the classic problem solving approach of "if X is hard,find out which Y is easy, and find out how you turn X into Y". This is a general life skill when it comes to problem solving and I honestly don't practice it enough, but I practiced it here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pulling markdown into JSX is hard,&lt;/li&gt;
&lt;li&gt;pulling JSX into JSX is trivial,&lt;/li&gt;
&lt;li&gt;I should convert markdown strings into JSX strings.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I'm pretty good at programming, and both markdown and JSX are, at their code, just string data in a file. As converting string data into other string data is a pretty easy thing if you know how to program, I wrote a script called &lt;code&gt;make-locales.js&lt;/code&gt; which runs through the &lt;code&gt;./components/sections&lt;/code&gt; directories looking for &lt;code&gt;content.*.md&lt;/code&gt; files, filters the list of locales it finds that way, turning it into a list of unique locales, and then for each locale in that list does something like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for (locale in locales) {
  giantMarkdownCollection = getAllContentFilesBelongingTo(locale)
  sectionAndContentMap = convertMarkdown(giantMarkdownCollection)
  convertedToJS = JSON.stringify(sectionAndContentMap)
  filesystem.write(`./locales/${locale}/content.js`, convertedToJS);
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Running this script builds a &lt;code&gt;content.js&lt;/code&gt; file that takes a form that matches the one necessary for any Node script (which JSX files are in my code base) to trivially import with a single &lt;code&gt;require('content')&lt;/code&gt; statement. By further making sure the data inside &lt;code&gt;content.js&lt;/code&gt; is keyed in the same way as the original code base organised sections, I basically had a markdown-to-JSX conversion that the original code base didn't even notice was different. Everything basically worked the same as far as it was concerned.&lt;/p&gt;
&lt;h3 id="further-challenges-i-m-not-using-true-markdown"&gt;Further challenges: I'm not using &lt;em&gt;true&lt;/em&gt; markdown&lt;/h3&gt;
&lt;p&gt;Of course, while the &lt;code&gt;get all content&lt;/code&gt; and &lt;code&gt;stringify&lt;/code&gt; operations are pretty easy, the crucial function to get right was that &lt;code&gt;convertMarkdown&lt;/code&gt; function, to turn the markdown syntax into JSX syntax instead. Thankfully, JSX syntax is basically JavaScript with embedded "HTML that follows XML rules", and converting markdown to HTML is super easy: just pick any of twenty or so libraries to do so, and you're essentially done.&lt;/p&gt;
&lt;p&gt;I picked the &lt;code&gt;marked&lt;/code&gt; library to do things for me, but there was one real challenge that needed to be tackled: the content I write is a mix of "mostly normal text", "some LaTeX, sometimes", "some divs with specific classes to mark bits as notes, how-to-code, and figures, sometimes" and some JSX for each interactive graphic... also sometimes. And being a pure markdown converter, except for the "normal text" parts &lt;code&gt;marked&lt;/code&gt; kind of didn't like any of that, so how would one make &lt;code&gt;marked&lt;/code&gt; convert things properly? &lt;/p&gt;
&lt;p&gt;Same problem solution process: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Converting mixed Markdown content is hard,&lt;/li&gt;
&lt;li&gt;converting just plain markdown is trivial,&lt;/li&gt;
&lt;li&gt;only convert plain markdown and leave the other bits alone.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The crucial observation was that in the build system I already had, things like "LaTeX", "divs with classes for notes and howtos" and "JSX" already worked. So really the only thing that &lt;em&gt;needed&lt;/em&gt; additional work was turning the markdown string sections into html string sections. &lt;/p&gt;
&lt;p&gt;Easy-peasy: I know how to write tokenizers, lexers and grammar parsers in general so I wrote a simple chained chunker that takes a markdown file, and then runs a super simple "chop it up, if I know how to chop it up" action.&lt;/p&gt;
&lt;p&gt;Start with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data = a full markdown file,
chunks = empty list to fill with data chunks,
chunkers = a list of latex, div, JSX, and BadMarkDown chunkers.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;define a function to act as recursion point:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;function performChunking(data, chunks, chunker, moreChunkers) {
  if no chunker:
    if data isn't empty:
      chunks.push({ convert: true, data: data })
    return;

  // otherwise, if there is a chunker:
  chunker(data, chunks, moreChunkers);
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and then finally, you just start blindly running through the data:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;function chunkLatex(data, chunks, chunkMore) {
  // run through the data looking for LaTeX blocks
  while there is data left to examine:
    if there is no latex left:
      performChunking(data.substring(p), chunks, next, otherChunkers);
      exit the chunkLatex function

    if there is, get the start of the latex block.
    Then, parse the non-LaTeX data prior to it using the rest of the chunkers:
        performChunking(data.substring(...), chunks, next=chunkMore[0], chunkMore=chunkMore[1,...])

    And then capture the LaTeX block itself as a "don't convert" block
    chunks.push({ convert: false, type: "latex", start:..., end:..., data:...});
  }
}

function chunkDiv(data, chunks, chunkMore) {
  Same as above, except for &amp;lt;div&amp;gt; and &amp;lt;/div&amp;gt; delimiters
}

function chunkJSX(data, chunks, chunkMore) {
  Same as above, except for &amp;lt;Graphic..../&amp;gt; lines
}

...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And so forth. This system ensures that a block that has no latex gets further analysed by the "div" code. Any divs are extracted, any non-div code is handed on to the JSX code, and so on and so on until there is no function left to examine with. At that point, we know it's just plain markdown and we record it as a "convert? yes!" data block.&lt;/p&gt;
&lt;p&gt;At the end of this process (which actually runs &lt;em&gt;really&lt;/em&gt; quickly), we end up with an object that looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;chunked = [
  {
    start: 0,
    end: 312,
    type: "text"
    convert: true,
    data: "# section title\nThis is regular markdown..."
  },
  {
    start: 313
    end: 417,
    type: "LaTeX",
    convert: false,
    data: "\\[\nB(t) = (1-t)^3 + 3 \cdot (1-t)^2t + ... " 
  },
  ...
]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And so we simply run through this through a quick &lt;code&gt;map&lt;/code&gt; function where any data that is marked as "convert? no!" is left alone, any data that is marked as "convert? yes!" is converted by &lt;code&gt;marked&lt;/code&gt; from markdown to HTML data. Then we simply join all the blocks back up, and we actually have the kind of JSX that the original monolithic English article was already using. &lt;/p&gt;
&lt;p&gt;Winner!&lt;/p&gt;
&lt;h3 id="one-last-thing-javascript-bindings-still-needs-to-work-"&gt;One last thing: JavaScript bindings still needs to work.&lt;/h3&gt;
&lt;p&gt;While the above procedure works &lt;em&gt;really&lt;/em&gt; well, it left one problem: sections have interactive graphics, which are tied to individual React components. While components were single JSX files that was not a problem, but by pulling the content out I needed a way to make sure that JSX code like &lt;code&gt;&amp;lt;Graphics setup={this.setup} draw={this.draw}/&amp;gt;&lt;/code&gt; still had a correct understanding of which JavaScript object was supposed to be used when it made calls for &lt;code&gt;this.something()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The solution to this is actually the simplest, borderline trivial,  a bit silly, but super effective: as each mapped chunk is strictly speaking already valid JSX I just took the string data and wrapped it in more string data that just turned it into a function call, wrapping it in &lt;code&gt;function(handler) { return &amp;lt;section&amp;gt;&lt;/code&gt; at the start and &lt;code&gt;&amp;lt;/section&amp;gt;; }&lt;/code&gt; at the end, and making sure the JSX chunker replaced any &lt;code&gt;this&lt;/code&gt; with the word &lt;code&gt;handler&lt;/code&gt; instead. The result: code like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;content = [
  "whatis": function(handler) {
    return &amp;lt;section&amp;gt;
     ...
     &amp;lt;Graphics setup={handler.setup} draw={handler.draw} /&amp;gt;
     ...
    &amp;lt;/section&amp;gt;;
  }
  ...
];
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And there you have it. Rather than importing this and then using it directly, a component can now import this and then call the function, passing itself in as the "handler":&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var React = require("react");

var Locale = require("../../../lib/locale");
var locale = new Locale(__dirname);

return React.createClass({
  getDefaultProps() {
    return {
      title: locale.getTitle()
    };
  },
  setup() {
    ...
  }
  draw() {
    ...
  }
  render() {
    return locale.getContent(this);
  }
});
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Sorted: suddenly we have a code base that is super easy to localize. Just change the &lt;code&gt;content.{local}.md&lt;/code&gt; file, and the &lt;code&gt;make-locales.js&lt;/code&gt; script will take care of the rest. In fact, with an &lt;code&gt;npm&lt;/code&gt; task that watches for changes in &lt;code&gt;.md&lt;/code&gt; files so that &lt;code&gt;make-locales.js&lt;/code&gt; gets retriggered, and a webpack task for &lt;code&gt;js&lt;/code&gt; files in general, live development didn't even need any changing: it just works.&lt;/p&gt;
&lt;h2 id="so-what-about-that-latex-is-maths-notation-universal-"&gt;So what about that LaTeX? Is maths notation universal?&lt;/h2&gt;
&lt;h3 id="spoilers-not-if-there-s-english-in-it"&gt;spoilers: not if there's English in it&lt;/h3&gt;
&lt;p&gt;Here's the thing about localisation: you need to update &lt;em&gt;all&lt;/em&gt; the content to work for a specific locale. Translating all the English text to something like Chinese is great and all but if the graphics still have English in them, things get weird. And while at this point the contributors were being quite productive and translating sections at a time, some of the LaTeX blocks still had English in them. &lt;/p&gt;
&lt;p&gt;Wouldn't it be nice if in a Japanese-localized Primer, this:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://pomax.github.io/bezierinfo/images/latex/8090904d6448ed0c8e6151aecf62f361d51ead96.svg" alt="English LaTeX"&gt;&lt;/p&gt;
&lt;p&gt;looked like this instead?&lt;/p&gt;
&lt;p&gt;&lt;img src="https://pomax.github.io/bezierinfo/images/latex/98885bce8eeabb5a9bdddd12cd6cb4382115ad5c.svg" alt="Japanese LaTeX"&gt;   &lt;/p&gt;
&lt;p&gt;Of course the answer is "yes, that would be lovely, actually" and so I had a look at whether CJK fonts could be used in LaTeX setting. I have a &lt;a href="http://pomax.github.io/nrGrammar"&gt;background in Japanese&lt;/a&gt; and so I've written LaTeX with Japanese in it before, and my naive thought was to just put Japanese in the LaTeX, teach MathJax to use a Unicode font with Japanese support, and that would be that. Unfortunately, it turns out MathJax is not as flexible as "real" LaTeX engines, and it can't really deal with non-English text. So that basically meant I had to give up MathJax, and instead try to go for something else.&lt;/p&gt;
&lt;p&gt;And as it so happens, there is a perfect candidate for this job that I never used in the build system because it really is just "a desktop tool" but as MathJax was already getting invoked through an &lt;code&gt;exec&lt;/code&gt; call, now seemed a good time to see if it would be a feasible solution to the problem of localized LaTeX: XeLaTeX, that is to say LaTeX code written for the &lt;a href="https://en.wikipedia.org/wiki/XeTeX"&gt;XeTeX&lt;/a&gt; engine, which is basically a modern, utf-8 aware, system and OpenType font aware TeX engine. If TeX is the system that lets you do "beautiful typesetting for English", XeTeX is the completely from-the-ground-up rewrite of TeX that lets you do "beautiful typesetting in any language, with any font".&lt;/p&gt;
&lt;h3 id="back-to-real-latex-parsing-"&gt;Back to real LaTeX parsing.&lt;/h3&gt;
&lt;p&gt;Switching the build stop over from calling MathJax to calling XeLaTeX was essentially trivial thanks to &lt;code&gt;npm&lt;/code&gt; scripts and Node.js's &lt;code&gt;execSync&lt;/code&gt;, so the only "hard" thing I had to do was write a script that managed the chain of calls necessary to generate an SVG file, as XeLaTeX generates PDF files. Thankfully, there are all kinds of handy tools that become available when you use TeX Live (on OSX or Unix/Linux) or MiKTeX (on Windows), so the chain of utilities that I settled on looked like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;XeLaTeX to generate a PDF file on a huge page,&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;pdfcrop&lt;/a&gt; to isolate just the content in the resultant PDF,&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;pdf2svg&lt;/a&gt; to convert that cropped PDF into an SVG file, and&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;svgo&lt;/a&gt; to optimize the SVG that pdf2svg generated. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and the way XeLaTeX got its latex was the same as before. Rather than calling:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;"latex": "node run lib/mathjax --latex ... --hash ..."
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;I wrote a &lt;code&gt;tex-to-svg&lt;/code&gt; script invoked as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;"latex": "node run lib/tex-to-svg --latex ... --hash ..."
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And made it ingest LaTeX code as a base64-encoded string from the command line, along with the precomputed hash that I needed for the final SVG filename. The script takes the base64-encoded LaTeX, unpacks it into a pure LaTeX string, and then adds additional LaTeX instructions to make sure the result is a valid &lt;code&gt;.tex&lt;/code&gt; file that XeLaTeX can work with. This file is written to disk (synchronously) after which XeLaTeX is invoked with that file as input parameter. We run through the chain of utilities, and the final result is a tiny, self-contained &lt;code&gt;.svg&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;Once we have that file, &lt;code&gt;text-to-svg&lt;/code&gt; is done, &lt;code&gt;npm&lt;/code&gt; script execution returns to the latex-loader that has been patiently waiting for the whole LaTeX-to-SVG process to finish, and it then consults the &lt;code&gt;.svg&lt;/code&gt; file on disk to get the SVG image width and height. Those matter, because we need to know the exact dimensions that the SVG file will take up on the page, so that we can write an &lt;code&gt;&amp;lt;img src="..." width="..." height="..."/&amp;gt;&lt;/code&gt; tag into our JSX to ensure that the page preallocates the correct amount of space for the image. If we didn't, the page would constantly be reflowing as images got loaded in, and as we're dealing with 150+ images, that would be a horrible experience. &lt;/p&gt;
&lt;h3 id="a-final-optimization-"&gt;A final optimization:&lt;/h3&gt;
&lt;p&gt;Of course if we had to run this every single time we ran &lt;code&gt;npm run dev&lt;/code&gt; or &lt;code&gt;npm start&lt;/code&gt;, this process would be impossibly slow. As such, there is a shortcut in the &lt;code&gt;latex-loader&lt;/code&gt; code that for any block of LaTeX checks whether there already exists an &lt;code&gt;.svg&lt;/code&gt; file with the correct hashcode as filename. If there is, the loader bypasses the tex-to-svg steps entirely, and instead immediately grabs the &lt;code&gt;.svg&lt;/code&gt; file for extracting the width/height information, and then returns the necessary &lt;code&gt;&amp;lt;img.../&amp;gt;&lt;/code&gt; code.&lt;/p&gt;
&lt;h2 id="so-how-do-we-switch-languages-"&gt;So how do we switch languages?&lt;/h2&gt;
&lt;p&gt;The page is hosted on github through the gh-pages functionality, and so some of the traditional ways to switch locales are not actually as appealing. Putting the locale in the URL, for instance, is not quite as easy when you'd literally have to make a dir by that name and put a file in there.&lt;/p&gt;
&lt;p&gt;So initially I figured I'd just use a URL query argument: &lt;code&gt;index.html?locale=en-GB&lt;/code&gt; would make a bit of script in the index file load the appropriate app bundle, and this would be sufficient for effecting different locales. That works really well, but one of the nice things about the Primer is that if you want an offline copy, you can literally just say "file -&amp;gt; save" and just save the page to your desktop. Done, you can now run it from file:// in the browser and it just works, and using query argument parsing to selectively load &lt;code&gt;article.{locale}.js&lt;/code&gt; would remove that ability.&lt;/p&gt;
&lt;p&gt;So directories it is: it's not clean from a dir structure perspective, but ultimate it's not the repo dir structure that &lt;em&gt;readers&lt;/em&gt; care about. If their needs are not met, then it doesn't matter how clean the dir structure is.&lt;/p&gt;
&lt;p&gt;Of course, you also need to be able to &lt;em&gt;switch&lt;/em&gt; locales on the site, so I wrote a small component that offers users a compellingly simple choice:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Read this in your own language: | &lt;strong&gt;English&lt;/strong&gt; | &lt;strong&gt;日本語&lt;/strong&gt; | &lt;strong&gt;中文&lt;/strong&gt; |
&lt;em&gt;Don't see your language listed? &lt;a href="https://github.com/Pomax/BézierInfo-2/wiki/localize"&gt;Help translate this content!&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A simple list of languages that people can read if it's theirs, and a call-out to anyone who &lt;em&gt;wants&lt;/em&gt; localized content but can't find it: this is an Open Source project, come help out!&lt;/p&gt;
&lt;h2 id="finally-document-everything"&gt;Finally: document everything&lt;/h2&gt;
&lt;p&gt;So now the code base can be localized! Hurray!&lt;/p&gt;
&lt;p&gt;Last question: can people read up on how to do that without needing to ask questions, and just &lt;em&gt;do it&lt;/em&gt;? If not, you're not done. Yes, it's great to have everything in place but unless you also write the documentation that explains how to do what people want to do, they're going to need help, and you probably won't have time to spend on helping them, so: write the docs that take you out of the equation.&lt;/p&gt;
&lt;p&gt;I ended up documenting &lt;a href="https://github.com/Pomax/BezierInfo-2/wiki/localize"&gt;the steps necessary to do localization&lt;/a&gt; on the github repo wiki, with links to the docs from the README.md, so that anyone visiting the repo --linked prominently on the Primer itself-- will immediately be able to find out what is involved and how they can make that work for them.&lt;/p&gt;
&lt;h2 id="so-what-s-left-"&gt;So what's left?&lt;/h2&gt;
&lt;p&gt;There is still one area of localization left to be tackled, and it's a big one with lots of question marks: localizing the actual interactive graphics. The problem with these is that text in these graphics are literally that: text in the browser.&lt;/p&gt;
&lt;p&gt;Unfortunately, text in the browser needs a reliable font for it to typeset even moderately predictably, and this poses two problems:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;we don't know the dimensions of localized strings, and&lt;/li&gt;
&lt;li&gt;we would need web fonts to ensure the right font gets used.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first of these is typically understood as a problem: where something might require four words in one language, it might require twenty in another, or maybe just one. If the text is to be used in a graphics setting, then those differing lengths pose all kinds of alignment problems.&lt;/p&gt;
&lt;p&gt;The second is less often remembered, but for CJK languages is a &lt;em&gt;huge&lt;/em&gt; problem: English fonts aren't very big compared to the modern website payload. 30kb for a nice looking font isn't even remarked upon anymore these days. But what about a 5+MB font just to support a few Japanese sentences use in graphics? or a 10+MB font to get Chinese text to show properly? And those are entirely reasonable font sizes for CJK languages: the smallest Japanese font I own is 4MB, and the smallest Chinese font I own is 12MB. The nice-looking fonts used for the LaTeX code are 7.7MB for Japanese and 5.1MB for Chinese. We clearly can't use those on the web.&lt;/p&gt;
&lt;p&gt;Or maybe we can: we could analyse all the content to see exactly which letters are &lt;em&gt;actually&lt;/em&gt; used, and then create subset fonts so that we have small fonts again, tailored for each localization. Which in turns means being able to analyse graphics for the text they use and then per locale recording exactly which glyphs should be available. It's a bit of work, but entirely doable given some time, leaving us just with the first problem.&lt;/p&gt;
&lt;p&gt;There's also some smaller stuff, like "how do you number sections properly?" using locale-specific CSS, "how do I make sure people understand that I am English and so won't be able to respond to non-English comments in the comments section?", and even the crazier things like "which social media links do you offer to Japanese or Chinese users? How do you localize that?"&lt;/p&gt;
&lt;p&gt;I don't have a solutions here, yet, but maybe that's the point of this post...&lt;/p&gt;
&lt;h2 id="localization-is-hard"&gt;localization is hard&lt;/h2&gt;
</description>
<category>Localization</category>
<category>Bezier</category>
<category>Primer</category>
<category>Contributors</category>
<link>http://pomax.github.io/#gh-weblog-1489108158510</link>
<guid>http://pomax.github.io/#gh-weblog-1489108158510</guid>
<pubDate>Fri, 10 Mar 2017 01:09:18 GMT</pubDate>
</item>
<item>
<title>I built a spice rack to fit our 72 mason jars of spices and herbs</title>
<description>&lt;p&gt;And it didn't even take all that long to do! It also took a bit to properly document so rather than repeat information on the internet, I will link to where I photo-documented the fabrication process and you can give that a read-through in case you ever want to do this yourself.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;78 image step-by-step documented album on imgur: &lt;a href="http://imgur.com/gallery/FIb3S"&gt;http://imgur.com/gallery/FIb3S&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Imgur comments with many good questions and pieces of advice: &lt;a href="http://imgur.com/gallery/FIb3S#comments"&gt;http://imgur.com/gallery/FIb3S#comments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Reddit comments with many good questions and pieces of advice: &lt;a href="https://www.reddit.com/r/DIY/comments/5e2m5x/i_made_a_spice_rack_to_fit_72_mason_jars_worth_of"&gt;https://www.reddit.com/r/DIY/comments/5e2m5x/i_made_a_spice_rack_to_fit_72_mason_jars_worth_of&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/pwwAkNA.jpg" alt="The finished spice rack"&gt;&lt;/p&gt;
</description>
<category>DIY</category>
<category>Spice Rack</category>
<category>Assembly</category>
<category>Carpentry</category>
<link>http://pomax.github.io/#gh-weblog-1480700902037</link>
<guid>http://pomax.github.io/#gh-weblog-1480700902037</guid>
<pubDate>Fri, 02 Dec 2016 17:48:22 GMT</pubDate>
</item>
<item>
<title> If you use use document.write, you suck at JavaScript</title>
<description>&lt;p&gt;Wow, that's an incendiary post title isn't it. Just because you use &lt;code&gt;document.write&lt;/code&gt; doesn't say anything about your JavaScript skills, I mean it's just another function in the official ECMAScript spec, it's just an older one. Why the vitriol?&lt;/p&gt;
&lt;h2 id="let-s-start-at-the-start"&gt;Let's start at the start&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;document.write&lt;/code&gt; function comes to us from the dark ages of the early JavaScript enabled web, and is really nothing like the JS you are likely to be familiar with.  It might be called &lt;code&gt;document.write&lt;/code&gt; and invoked as if it's part of the document API, &lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/Document/write"&gt;its true behaviour is much lower level&lt;/a&gt;: it is a proxy function in to the bytesteam pipe that defines what's even &lt;strong&gt;in&lt;/strong&gt; your &lt;code&gt;document&lt;/code&gt;. The &lt;code&gt;write&lt;/code&gt; function doesn't let you "write some data into the document", it &lt;strong&gt;is&lt;/strong&gt; the document, and that comes with some hilarious side effects.&lt;/p&gt;
&lt;h2 id="1-write-needs-an-open-pipe"&gt;1: &lt;code&gt;write&lt;/code&gt; needs an open pipe&lt;/h2&gt;
&lt;p&gt;Say you have a web page, and that webpage has the following code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;html&amp;gt;
  &amp;lt;head&amp;gt;&amp;lt;title&amp;gt;whatever&amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt;
  &amp;lt;body&amp;gt;
    &amp;lt;script&amp;gt; document.write("&amp;lt;p&amp;gt;oh look a paragraph&amp;lt;/p&amp;gt;"); &amp;lt;/script&amp;gt;
  &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The browser loads this code by creating a &lt;code&gt;document&lt;/code&gt;, opening it for writing, piping that data into it (which gets parsed into a DOM as it sees bytes flying by) and then closes it to signal the completion of building &lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model"&gt;the DOM&lt;/a&gt; for your page.&lt;/p&gt;
&lt;p&gt;Given that, you might expect this code to write a paragraph into your document at the time that the &lt;code&gt;document.write&lt;/code&gt; script triggers. And you'd be right, but here is &lt;em&gt;why&lt;/em&gt; you're right: as the document pipe is still open, and the script element gets evaluated &lt;em&gt;while&lt;/em&gt; the document is getting parsed, the document parser simply sees "more bytes" flying by to parse into the DOM.&lt;/p&gt;
&lt;p&gt;Want to see something hilarious?&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;html&amp;gt;
  &amp;lt;head&amp;gt;&amp;lt;title&amp;gt;whatever&amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt;
  &amp;lt;body&amp;gt;
    &amp;lt;script&amp;gt;
      document.addEventListener("DOMContentLoaded", function() {
        document.write("&amp;lt;p&amp;gt;oh look a paragraph&amp;lt;/p&amp;gt;");
      });
    &amp;lt;/script&amp;gt;
  &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Guess what that does. If you thought "it writes a paragraph into the document once it's finished loading" then: no, not really.  While it might look that way if you put this in jsbin or the like that's what it looks like it does, but let's try this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;html&amp;gt;
  &amp;lt;head&amp;gt;&amp;lt;title&amp;gt;whatever&amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt;
  &amp;lt;body&amp;gt;
    &amp;lt;h1&amp;gt;Second test:&amp;lt;/h1&amp;gt;
    &amp;lt;script&amp;gt;
      document.addEventListener("DOMContentLoaded", function() {
        document.write("&amp;lt;p&amp;gt;oh look a paragraph&amp;lt;/p&amp;gt;");
      });
    &amp;lt;/script&amp;gt;
  &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Run this code. Seriously, hit up &lt;a href="https://jsbin.com"&gt;jsbin&lt;/a&gt; and paste that into the HTML section, then run it. See what happens.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Huh, where did that &lt;code&gt;&amp;lt;h1&amp;gt;&lt;/code&gt; go?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Actually, the correct question is "where did &lt;strong&gt;everything&lt;/strong&gt; go?" because your entire document got wiped. After all, &lt;code&gt;document.write&lt;/code&gt; needs an open pipe, and there is no open pipe once all content has loaded. And so it makes one.&lt;/p&gt;
&lt;p&gt;A new one.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;An empty one&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;document&lt;/code&gt; is now an empty, open data pipe&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;What happened? The document got opened, the source code bytes went flying past the parser, it saw the end of the document, the document got closed for writing, and then when it sends the signal "DOM finished building!", our &lt;code&gt;document.write&lt;/code&gt; code kicks in, and the document is reopened as a new, empty, byte stream, and we pump in only the bytes &lt;code&gt;&amp;lt;p&amp;gt;oh look a paragraph&amp;lt;/p&amp;gt;&lt;/code&gt;. The browser knows how to auto-inject missing tags so it'll automatically build the higher level &lt;code&gt;html&lt;/code&gt;, &lt;code&gt;head&lt;/code&gt; and &lt;code&gt;body&lt;/code&gt; nodes in the DOM, but the &lt;em&gt;only&lt;/em&gt; data in our document as far as the browser knows is now &lt;code&gt;&amp;lt;p&amp;gt;oh look a paragraph&amp;lt;/p&amp;gt;&lt;/code&gt;, and &lt;strong&gt;nothing else&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So congratulations, you actually just wiped the page! And this is not a "quirk", this is literally what it's supposed to do, it's intended and explicit behaviour. If this was code you wrote on your own page, then lucky you, easy to fix (don't use &lt;code&gt;document.write&lt;/code&gt;) but if this was a 3rd party library, good luck finding out which one is  responsible. And then replacing it with a modern library instead.&lt;/p&gt;
&lt;h2 id="2-document-write-is-insanely-insecure-"&gt;2.  &lt;code&gt;document.write&lt;/code&gt; is insanely insecure.&lt;/h2&gt;
&lt;p&gt;You probably know that &lt;code&gt;eval()&lt;/code&gt; is really bad because it can evaluate arbitrary JS code, with the same execution rights as the page it's run on. So... you should see this one coming, but &lt;code&gt;document.write&lt;/code&gt; lets you do exactly the same thing. In fact, it's the most low-level version of eval you can come up with: it's &lt;em&gt;literally&lt;/em&gt; the function for injecting data into a page, so let's do that...&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;html&amp;gt;
  &amp;lt;head&amp;gt;&amp;lt;title&amp;gt;whatever&amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt;
  &amp;lt;body&amp;gt;
    &amp;lt;script src="https://third.party.domain/perfectly-safe.js"&amp;gt;&amp;lt;/script&amp;gt;
  &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and let's put a &lt;code&gt;document.write&lt;/code&gt; in that "perfectly safe" js file!&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;document.write("&amp;lt;script&amp;gt; do.anything(); &amp;lt;/script&amp;gt;");
/*
  awesome,  this injected code will run as if it's part of the page this is run on,
  not our http://third.party.domain context, so it will have access to everything
  security related that we shouldn't ever have access to!
*/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Good times!&lt;/p&gt;
&lt;h2 id="3-document-write-is-synchronous"&gt;3.  &lt;code&gt;document.write&lt;/code&gt; is synchronous&lt;/h2&gt;
&lt;p&gt;Because we can't defer &lt;code&gt;document.write&lt;/code&gt; to after the document is done loading, all the &lt;code&gt;document.write&lt;/code&gt; code &lt;em&gt;has&lt;/em&gt; to run as part of the initial page load, and so anything it puts in your document increases time-to-load. If you are blessed enough to live in a part of the world where you've never noticed page load speed, good for you! But a &lt;em&gt;lot&lt;/em&gt; of people are still on slow connections, and any page blocking before the browser can show an initial view (which it only does once it reaches the &lt;code&gt;&amp;lt;/body&amp;gt;&lt;/code&gt; tag) is extremely obvious.&lt;/p&gt;
&lt;h2 id="okay-fine-but-say-i-know-all-these-things-why-do-i-suck-at-javascript-if-i-use-it-i-know-what-i-m-doing-"&gt;Okay, fine, but say I know all these things, why do I suck at JavaScript if I use it? I know what I'm doing!&lt;/h2&gt;
&lt;p&gt;I'm going to  argue that you don't. Not necessarily through any fault of your own, but you can still suck at something even if you don't realise you do.&lt;/p&gt;
&lt;p&gt;Modern JS has many constructions that are non-destructive, secure, and non-blocking. Why not use those instead? And that's a rhetorical question, really: you &lt;em&gt;should&lt;/em&gt; be using the modern JS equivalents, not a low-level document bytecode writer. It doesn't really matter what you want to do, because I know what you don't want to do: you don't want to "write byte data into the document", you want to do something else.&lt;/p&gt;
&lt;p&gt;As such, like any proper solution to &lt;a href="http://meta.stackexchange.com/questions/66377/what-is-the-xy-problem"&gt;an XY problem&lt;/a&gt;, solve the right problem with the right tools. In this case, do what you &lt;em&gt;actually&lt;/em&gt; want to get done using the appropriate modern APIs for doing that thing; don't resort to &lt;code&gt;document.write&lt;/code&gt; as some kind of magical hammer, because then you're just abusing &lt;code&gt;document.write&lt;/code&gt; to do something it wasn't designed to do, in a way that is horrendously broken and bug-prone.&lt;/p&gt;
&lt;p&gt;So what do you use instead?&lt;/p&gt;
&lt;h3 id="adding-content-to-the-document"&gt;Adding content to the document&lt;/h3&gt;
&lt;p&gt;The proper way to add content to your document, using modern, non-destructive code, is by using the DOM manipulation API. In vanilla JS, things like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;html&amp;gt;
  &amp;lt;head&amp;gt;&amp;lt;title&amp;gt;whatever&amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt;
  &amp;lt;body&amp;gt;
    &amp;lt;h1&amp;gt;Second test:&amp;lt;/h1&amp;gt;
    &amp;lt;script&amp;gt;
      document.addEventListener("DOMContentLoaded", function() {
        var body = document.body.
        var p = document.createElement("p");
        p.textContent = "wow, a new paragraph!";
        body.appendChild(p);
      });
    &amp;lt;/script&amp;gt;
  &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Is that cumbersome? Sure is, so if you have to manipulate the DOM, use something like jQuery or Zepto or the like instead, because they give you far easier APIs, while making sure to fall through to proper DOM manipulation instead of &lt;code&gt;document.write&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;html&amp;gt;
  &amp;lt;head&amp;gt;&amp;lt;title&amp;gt;whatever&amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt;
  &amp;lt;body&amp;gt;
    &amp;lt;h1&amp;gt;Second test:&amp;lt;/h1&amp;gt;
    &amp;lt;script&amp;gt;
      $(function() {
        $(document.body).append($("&amp;lt;p&amp;gt;Wow, a new paragraph!&amp;lt;/p&amp;gt;");
      });
    &amp;lt;/script&amp;gt;
  &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Done.  And it runs asynchronous, too, so your page isn't blocked from loading.&lt;/p&gt;
&lt;h3 id="but-i-only-use-document-write-to-see-debug-information-"&gt;But I only use &lt;code&gt;document.write&lt;/code&gt; to see debug information!&lt;/h3&gt;
&lt;p&gt;Okay... wait, what? Why are you doing that? It's 2016, but even if this was 2010: use the &lt;code&gt;console&lt;/code&gt; API, because that's what it's for.&lt;/p&gt;
&lt;p&gt;You open your browser's dev tools view, and you use &lt;code&gt;console.log(...)&lt;/code&gt; to write data to the dev tools console. In fact, unlike &lt;code&gt;document.write&lt;/code&gt; which can only write strings (obviously), &lt;code&gt;console.log&lt;/code&gt; can write everything you throw at it, and can write as many things as you need written in a single command. Strings, arrays, full objects, it makes all those things accessible in the console in an explorable way, and in a single call: &lt;code&gt;console.log("test", [1,2,3], {x:0,y:undefined})&lt;/code&gt; just works.&lt;/p&gt;
&lt;p&gt;In fact, because being explicit about your intent in code is important for your code to make sense not just to others but even to your future self, the console API even has things like &lt;code&gt;console.warn()&lt;/code&gt; and &lt;code&gt;console.error()&lt;/code&gt;, so you can be specific about what your debug is for (just "see the data" vs "strong signal that something's wrong"). And if you want to get &lt;em&gt;really&lt;/em&gt; fancy there's things like &lt;code&gt;console.table()&lt;/code&gt; for pretty-printing specific data in tabulated form. &lt;/p&gt;
&lt;p&gt;The console is amazing and you should be using it. Not &lt;code&gt;document.write&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id="but-what-if-i-use-it-for-"&gt;But what if I use it for--&lt;/h3&gt;
&lt;p&gt;I kind of stopped caring: whatever you're doing, there is a modern JS way to do that without using &lt;strong&gt;a bare-bones byte injection function hooked into the normally inaccessible document byte map&lt;/strong&gt;. &lt;/p&gt;
&lt;h3 id="no-really-i-m-using-it-for-injecting-a-filesystem-loader-into-an-iframe-that-kicks-in-before-the-document-even-exists-replacing-it-with-a-posix-filesystem-enabled-mini-server-that-"&gt;No really, I'm using it for injecting a filesystem loader into an iframe that kicks in before the document even exists, replacing it with a POSIX filesystem-enabled mini server that--&lt;/h3&gt;
&lt;p&gt;That sounds awesome, I did the same thing! Also, that's an amazeballs super-ninja insane-o-hack, so I hope you're not counting on that working forever, because &lt;code&gt;document.write&lt;/code&gt; has to go. No matter how neat the thing is we can achieve by abusing the hell out of it. Our exploits of it are not enough to justify keeping it around.&lt;/p&gt;
&lt;p&gt;Seriously: &lt;em&gt;&lt;code&gt;document.write&lt;/code&gt; has to go.&lt;/em&gt;&lt;/p&gt;
&lt;h1 id="do-you-really-suck-at-javascript-probably-not-"&gt;Do you &lt;em&gt;really&lt;/em&gt; suck at JavaScript? Probably not.&lt;/h1&gt;
&lt;p&gt;So back to the original proposition. Do you suck at JavaScript? Well, sort of. You're using a function you shouldn't ever have been taught to use. That happens. We all started at a position of "knowing nothing about JavaScript", so not knowing how bad &lt;code&gt;document.write&lt;/code&gt; is, is kind of a given. However, now you know, and now you need to go and find all those places where you use it.&lt;/p&gt;
&lt;p&gt;And then do you suck at JavaScript? Well, that depends on what you do next: if you don't stop using &lt;code&gt;document.write&lt;/code&gt; now, and you don't go an correct your past mistakes, then yes; you absolutely suck at JavaScript, and the web world is worse off because of you. No two ways about it: if that's you, you're part of the problem. And not even because of &lt;code&gt;document.write&lt;/code&gt; anymore but because you are unwilling to fix obviously bad behaviour. You have problems well outside of writing JavaScript.&lt;/p&gt;
&lt;p&gt;So shape up, and help everyone learn better JS by writing better JS. You have the skills and the power, exercise them, make a diffrence.&lt;/p&gt;
&lt;p&gt;And to all of you already avoiding &lt;code&gt;document.write&lt;/code&gt; like the low-level legacy function that it is: good on you, keep up the good work. Maybe convince a friend to do the same, tweet about it, write a blog post; together we can fix the web.&lt;/p&gt;
</description>
<category>JavaScript</category>
<category>Web dev</category>
<category>document.write</category>
<link>http://pomax.github.io/#gh-weblog-1473270609919</link>
<guid>http://pomax.github.io/#gh-weblog-1473270609919</guid>
<pubDate>Wed, 07 Sep 2016 17:50:09 GMT</pubDate>
</item>
<item>
<title>So... I ruined my hands (but they're getting better)</title>
<description>&lt;p&gt;About two weeks ago, I was fixing my favourite ramen bowl using traditional Japanese pottery repair techniques, which involves working with "urushi", a natural laquer obtained from the &lt;a href="https://en.wikipedia.org/wiki/Chinese_laquer_tree"&gt;Chinese Laquer Tree&lt;/a&gt;. This might not mean anything to you, until I explain to you that there is a compound known to science called "urushiol" that is found in poison ivy, poison oak, poison sumac, cashew trees, and other such nasty plants, and that is causes massive reactions in a large portion of humans.&lt;/p&gt;
&lt;p&gt;Urushi is what that compound is named after, and while I wore gloves while working on my ramen bowl, I took them off before cleaning up. And so I ended up with &lt;a href="https://en.wikipedia.org/wiki/Urushiol-induced_contact_dermatitis"&gt;urushiol-induced contact dermatitis&lt;/a&gt;. The nasty thing about it is that it doesn't kick in immediately. As an antigen reaction, the first time this happens it takes one to two weeks before your body reacts to it, but like any good antigen reaction, the second time it only takes a few days. However, a "a few days" is not "immediately" and so you don't realise you screwed up until three days later, when it's too late: this is happening, better make your peace with it.&lt;/p&gt;
&lt;p&gt;And making your peace with it is hard: the affected skin will start to rash and blister, and those blisters will just get bigger and bigger, spawning blisters INSIDE of them, and joining up to form huge finger-spanning blisters with smaller blisters inside of them. If that sounds horrible, good news: it's even worse because it also itches like &lt;em&gt;mad&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;There are some ways to mitigate it, and none of them are drugs or creams: the most effective way to deal with the itching is to soak the affected region (if possible) in super hot water. As hot as you can stand, and then just a fraction hotter. It desensitizes the tissue and alleviates the itching for at least a few hours. And after a week, the blisters start to recede again once the body finally figures out that none of its defenses are actually doing anything (no kidding, if only we could tell it that earlier), and then the rest of your body starts to catch up: any part of your body that undergoes repeated skin stress (bits that chafe up against clothing, a scrape you got from walking into a door handle, and so on) develops a rash, too. No blisters, but you'll enjoy swelling and rashing and massive itchiness a second time over.&lt;/p&gt;
&lt;p&gt;Now, that sounds horrible, and it kind of is, but the most annoying part is that once you realise what's happening, it's a) too late and b) you're going to lose the functionality of whatever got the blister part of the deal. In my case, my left hand blistered up so badly that I couldn't use it, and my right was pretty close too. I ended up buying curator's gloves just so that moving my fingers ever so slightly wouldn't make blisters rub up against blisters, causing itching and pain.&lt;/p&gt;
&lt;p&gt;Does that sound gross? Good: &lt;strong&gt;wear gloves when working with urushi, and wear clothes if you're in a poison ivy/poison oak region&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Seriously, it'll take you down, and if you live in North America, your doctor's just going to prescribe antibiotics and a topical cream, neither of which will actually do anything to make you heal faster - they're just overreactions to "you might scratch yourself open and get an infection O_O" (protip: don't scratch. Yeah it itches like crazy, suck it up. 2 weeks from now you won't remember itching 2 weeks ago). You have to ride it out, and you have to not scratch, and you will be driven close to mad, while not being able to do your job.&lt;/p&gt;
&lt;p&gt;Don't be like me.&lt;/p&gt;
</description>
<category>Urushi</category>
<category>Poison Ivy</category>
<category>Poison Oak</category>
<category>Safety</category>
<link>http://pomax.github.io/#gh-weblog-1469494036040</link>
<guid>http://pomax.github.io/#gh-weblog-1469494036040</guid>
<pubDate>Tue, 26 Jul 2016 00:47:16 GMT</pubDate>
</item>
<item>
<title> Live commentary is soul-destroying</title>
<description>&lt;p&gt;I started a Youtube channel called &lt;a href="https://www.youtube.com/channel/UC_vhEBO8O9ABn1f32KuUtjw"&gt;Baking McCookery&lt;/a&gt; a while ago for recording things I make in the kitchen, and for a while these were overhead and stove videos with text overlays, and that worked rather well, but they were silent, and that doesn't work all that well.&lt;/p&gt;
&lt;p&gt;As such, people suggested getting a mic and talking while cooking, to capture more of the process. And that sounded like a good idea! So I did - I bought a &lt;a href="https://www.amazon.ca/gp/product/B01EWSD17E"&gt;cheap(ish) voice recorder&lt;/a&gt; and decent &lt;a href="https://www.amazon.ca/gp/product/B00PQYMFU8"&gt;lavalier microphone&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Except having ruined five recording sessions now over audio problems, clipped signals, stutters, and general commentary problems that sync up with the video in a way that they can't be cut out has actually left me extremely depressed over the whole venture, and that was never the point of these videos: this was supposed to be fun.&lt;/p&gt;
&lt;p&gt;Working in the kitchen is a thing that directly influences my emotional state: making delicious food makes me happy, and ruining food gets me angry and depressed, and spending hours in a kitchen only to end up with failures this many times means I'm doing this wrong: I've allowed a hobby to become a depression, and it's time to cut that off.&lt;/p&gt;
&lt;p&gt;So: after ending up with only a single session that was even remotely passable (and even then, had clipped audio - I might eventually redo it without audio and just consider that playlist a lost cause to be deleted) I'm going back to text overlays. Sure, I might add some background music, and I will think about whether post-commentary is an option, but for now I want to enjoy my life in the kitchen again. And that means overhead and stove videos that let me &lt;a href="http://wouldeatagain.ca/2016/07/10/chocolate-bread-pudding"&gt;do what I love most&lt;/a&gt;, without having to divide my focus between "doing the thing I love" and "talking about the thing I'm doing".&lt;/p&gt;
</description>
<category>Youtube</category>
<category>Videography</category>
<category>Quality of Life</category>
<link>http://pomax.github.io/#gh-weblog-1468256151147</link>
<guid>http://pomax.github.io/#gh-weblog-1468256151147</guid>
<pubDate>Mon, 11 Jul 2016 16:55:51 GMT</pubDate>
</item></channel>
</rss>
