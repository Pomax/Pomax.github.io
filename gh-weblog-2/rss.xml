<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<atom:link href="http://pomax.github.io/gh-weblog-2/rss.xml" rel="self" type="application/rss+xml" />
<title>Pomax.github.io</title>
<description>My blog on github</description>
<link>http://pomax.github.io</link>
<lastBuildDate>Thu, 23 Jun 2016 21:32:21 GMT</lastBuildDate>
<pubDate>Thu, 23 Jun 2016 21:32:21 GMT</pubDate>
<ttl>1440</ttl>
<item>
<title>Mobile photography is a joke</title>
<description>&lt;p&gt;I've written about &lt;a href="http://pomax.github.io/1436836360570/we-are-really-terrible-at-digital-colours-and-digital-photography"&gt;how bad we are at digital photography from a technical perspective&lt;/a&gt; before, but now that I own a high end mobile device with a decent camera and screen (I was using a &lt;a href="https://en.wikipedia.org/wiki/Nokia_N900"&gt;Nokia N900&lt;/a&gt; for my phone needs until a few weeks ago. Now it's exclusively my alarm clock), it is a constant source of frustration how terrible photography on $700 and above mobile devices is. Certainly, you could argue that they're phones, not cameras, but then you've not really been paying attention to what people actually &lt;em&gt;use&lt;/em&gt; their phones for, where photography ranks considerably higher than calls.&lt;/p&gt;
&lt;p&gt;I have a &lt;a href="https://store.google.com/product/nexus_6p"&gt;Nexus 6P&lt;/a&gt; now, which is a high end Android device with —on paper— a rather good camera, and a very nice AMOLED screen. However, photographs taken with it, no matter whether that's done with the native camera app, &lt;a href="http://www.magix.com/ca/apps/cameramx/"&gt;CameraMX&lt;/a&gt;, &lt;a href="http://www.camerafv5.com/"&gt;Camera FV-5&lt;/a&gt;, or &lt;a href="http://opencamera.sourceforge.net/"&gt;Open Camera&lt;/a&gt;, unless I treat the phone as "half the work", none of the pictures it takes are worth anything if my intention is to share them with the rest of the world, because while I might see "nice, vibrant colours", the moment  I post that picture online people will see completely different colours than I'm getting on my phone. All the edits I made, even with $10 or $25 image processing apps, were nothing but a waste of time and potentially money unless I share it with other Nexus 6P users exclusively of course. And that's not happening for many and obvious reasons. &lt;/p&gt;
&lt;h2 id="colour-calibration-or-the-lack-thereof"&gt;Colour calibration - or, the lack thereof&lt;/h2&gt;
&lt;p&gt;If you care about your colours —and most photographers do, regardless of whether they're hobbyists or professionals— you need colour calibration. On desktop this means colour calibrating your monitor (either with an expensive calibrator like a &lt;a href="http://spyder.datacolor.com/portfolio-view/spyder5pro"&gt;SpyderPro&lt;/a&gt;, or with some &lt;a href="https://blog.adafruit.com/2011/11/14/colorhug"&gt;open source and open hardware&lt;/a&gt; and personal effort), so that the colours it displays are the colours you shot (constrained by the absolute nonsense that is our acceptance of &lt;a href="https://en.wikipedia.org/wiki/Gamut"&gt;color gamuts&lt;/a&gt;, where even in 2016 we collectively don't seem to care that screens literally cannot display about half the colours the human eye sees in the real world - where's universal &lt;a href="https://en.wikipedia.org/wiki/Ultra-high-definition_television"&gt;UHDTV&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Rec._2020"&gt;Rec. 2020&lt;/a&gt; support in regular consumer displays?).&lt;/p&gt;
&lt;p&gt;And, it means shooting a &lt;a href="https://www.google.ca/search?q=color+profile+target&amp;amp;tbm=isch"&gt;profiling target&lt;/a&gt; with your camera because calibration has to pass through every display chip: even if your monitor is calibrated, you still need to make sure that the color incorrectness of your camera can be compensated for as well (and oh boy can cameras be wrong)&lt;/p&gt;
&lt;p&gt;So on an Android "phone" we have the same problem, but in a single device. We need to be able to calibrate the screen, because those fancy, super bright, super clear AMOLED screens actually get colour &lt;strong&gt;super wrong&lt;/strong&gt;, and we need to be able to do shot calibration through the camera. &lt;/p&gt;
&lt;p&gt;Neither of those things are possible.&lt;/p&gt;
&lt;h2 id="calibrating-the-screen"&gt;Calibrating the screen&lt;/h2&gt;
&lt;p&gt;If you were smart enough to, after getting your Android device, immediately install an &lt;a href="https://source.android.com"&gt;AOSP&lt;/a&gt; firmware instead, or even something like &lt;a href="http://www.cyanogenmod.org"&gt;CyanogenMod&lt;/a&gt;, you might think you have a leg up, because your enriched version of Android actually comes with colour calibration. Except it really doesn't, all it does is come with global &lt;a href="https://en.wikipedia.org/wiki/Color_balance"&gt;colour &lt;em&gt;balancing&lt;/em&gt;&lt;/a&gt;: you have the ability to control what the &lt;a href="https://en.wikipedia.org/wiki/White_point"&gt;white point&lt;/a&gt; for your screen is, but that's it. If your screen's blues are a bit too red, and the reds are a bit too yellow, then no matter how much you fiddle with that white point, you're going to solve only one of those issues, while exacerbating the other.  Sure, it might make the AMOLED screen "less yellow", but that doesn't help us when it comes to making sure the colours we shoot with the device camera(s) match what they actually were (again, as constrained by the gamut we're forced to use).&lt;/p&gt;
&lt;p&gt;Real calibration measures the response curves for each of the primaries that a display can generate, as well as the intensity responses curves for "greys" (with each primary firing at the same supposed intensity), and builds an ICC profile that lets the device tell the display how to change primary intensities independently of one another. Unlike a white point shift, where the display essentially just renders colours with an overlay filter, without looking at individual pixel values, true colour calibration involves rewriting colour values for every individual pixel.&lt;/p&gt;
&lt;p&gt;And for reasons that I do not understand, &lt;a href="http://www.apple.com/ca/shop/buy-iphone/iphone6"&gt;$1000 imaging devices&lt;/a&gt; that you carry around in your pocket can't do that, whereas even a $25 graphics card in an ancient computer running Windows XP will happily apply an ICC profile for you. &lt;em&gt;Surely&lt;/em&gt; this is not a technical limitation at this point.&lt;/p&gt;
&lt;h2 id="calibrating-the-camera"&gt;Calibrating the camera&lt;/h2&gt;
&lt;p&gt;Camera calibration is more interesting because the only way to properly calibrate the output for a camera sensor is to take pictures of things with known colour values (still constrained to the gamut used for those colours values) and then compute the difference curves between "what the camera saw" and "what it should have reported seeing".&lt;/p&gt;
&lt;p&gt;One problem here is that your eye has been fooling you for as long as you've been alive, and your brain rarely, if ever, notices that it is supremely good at lighting condition compensation. Differences in material properties mean that two different objects under two different lighting conditions will reflect different colours, &lt;em&gt;non-uniformly&lt;/em&gt;: a matte white plaster and high gloss white plastic ball may reflect daylight in a way that lets us compute the difference between the white we think the objects should be, and the &lt;a href="https://en.wikipedia.org/wiki/Color_temperature"&gt;blueish&lt;/a&gt; colour that the objects actually are in sunlight. But if we change the lighting to halogen floodlights, the difference in absorption and reflection in the different materials means that the difference in colour between the two balls is &lt;em&gt;different&lt;/em&gt; from the difference in colour between them in sunlight.&lt;/p&gt;
&lt;p&gt;Color calibration is hard. &lt;/p&gt;
&lt;p&gt;This is why you'll see photographers carry around calibration targets: cards printed with specific colours on them that they'll periodically take pictures of during shoots so that during post processing they can ensure that for every shot under specific lighting conditions, there is a way to determine what the correction should look like. While display calibration is fairly static, scene calibration is a constant necessity.&lt;/p&gt;
&lt;p&gt;Unfortunately, even though there are a number of commonly used scene calibration targets, neither Android nor IOS have ways to tell the phone itself to take a calibration with "calibration target X" so that the camera data that gets sent to a camera app is correct, which means that scene calibration on a mobile device is still the same as in traditional photography:  take a picture of a calibration target every now and then, and then "fix things in post".&lt;/p&gt;
&lt;p&gt;Except if you just have a mobile device, there &lt;strong&gt;is&lt;/strong&gt; no reasonable post processing. There are no apps that perform batch corrects, there are no apps that can abstract a correction profile off of an &lt;a href="https://en.wikipedia.org/wiki/IT8"&gt;IT8&lt;/a&gt; or &lt;a href="http://xritephoto.com/colorchecker-passport-photo"&gt;ColorChecker&lt;/a&gt; target, there are no apps that can load or embed &lt;a href="https://en.wikipedia.org/wiki/ICC_profile"&gt;ICC profiles&lt;/a&gt; into images. There is nothing, and so the only way to do half-decent photography with a $1000 device that is both camera and computer, is to have a &lt;strong&gt;second&lt;/strong&gt; computer with a desktop operating system and real photo processing software.&lt;/p&gt;
&lt;h2 id="why-are-you-complaining-just-use-a-real-camera-then"&gt;Why are you complaining, just use a real camera then&lt;/h2&gt;
&lt;p&gt;Fair enough. I have several real cameras, but that's exactly why I'm going to invoke one of my favourite quotes in recent years in a moment: my phone costs $700; that's an insane amount of money if all it could do was call and text, and so it can do a lot more. It has a high performance CPU and GPU, it has a high quality screen, and two (not just one) high resolution, high quality fixed aperture cameras. And you're asking why I should expect to be able to do a thing that literally 100% of users of this kind of device do dozens of times a day, with a quality that reflects the price and quality of the hardware I'm using, instead of "in the gimpiest broken way possible"? I'll tell you why, and here's that quote:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;"Because this is bullshit, and we can fix this."&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;(Do you work on any Android camera apps? Please bake in profile support to your app. Do you work on Android? Please start taking colour seriously, people use phones to share images, not take pics and only ever view them on their own phone. Do you care about your friends seeing the same photo as the one you saw on your phone? Complain to the former two groups, or become part of them. As for me: the complaints will continue until &lt;s&gt;morale&lt;/s&gt; support improves)&lt;/p&gt;
</description>
<category>Mobile</category>
<category>Photography</category>
<category>Android</category>
<category>IOS</category>
<category>Color</category>
<category>Calibration</category>
<link>http://pomax.github.io/#gh-weblog-1466701364674</link>
<guid>http://pomax.github.io/#gh-weblog-1466701364674</guid>
<pubDate>Thu, 23 Jun 2016 17:02:44 GMT</pubDate>
</item>
<item>
<title>I started a Youtube channel.</title>
<description>&lt;p&gt;I like &lt;a href="https://imgur.com"&gt;imgur.com&lt;/a&gt;, it's a great way to both fill moments of downtime with some interesting pictures posts, as well as waste too much time trying to find the next picture post, but one of the things that gets posted with reasonable frequency is food pictures, and they're always these high speed, "it's so easy just do this!" recipe gifs. &lt;/p&gt;
&lt;p&gt;While I won't contest that there are easy recipes, provided you do your prep first, cooking -and baking- takes time, and I wanted to create something that shows off &lt;em&gt;that&lt;/em&gt; aspect of kitchen work.  So, I started a Youtube channel over on &lt;a href="https://www.youtube.com/channel/UC_vhEBO8O9ABn1f32KuUtjw"&gt;https://www.youtube.com/channel/UC_vhEBO8O9ABn1f32KuUtjw&lt;/a&gt;, starting with a video series on &lt;a href="https://www.youtube.com/playlist?list=PL5_wrS2aqN9YFaSCWbpo-OEvbytigJWMY"&gt;making baguette&lt;/a&gt;, start to finish. Baguette is not "easy", and it's not "fast"; any recipe that claims it is is either lying or selling you something (thanks "&lt;a href="http://www.imdb.com/title/tt0093779"&gt;The Princess Bride&lt;/a&gt;"!) but it is entirely doable, provided you follow all the steps, and all the steps take time. So I took mine and made 15 videos that document the process in real time, except for the "in the oven" videos, which are sped up because seeing what the dough does actually teaches you something.&lt;/p&gt;
&lt;p&gt;And since it would be a shame to just do one kitchen video series and then sell the cameras I used again, I'll be trying to post some new series every now and then. For now, that means there's a video series for &lt;a href="https://www.youtube.com/playlist?list=PL5_wrS2aqN9aPVEKij5gDLf91UoapIrWH"&gt;Romano Pizza&lt;/a&gt; and a video series for &lt;a href="https://www.youtube.com/playlist?list=PL5_wrS2aqN9Z7RYLc9-npfsMRH0hXUQut"&gt;Chicken Shepherd's Pie&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It's fun to do, obviously it's food that gets eaten after it gets made, and if you're tired of "an hour of work in 30 seconds" like I am, hit me up, I'm sure you have ideas I am interested in.&lt;/p&gt;
</description>

<link>http://pomax.github.io/#gh-weblog-1464027515789</link>
<guid>http://pomax.github.io/#gh-weblog-1464027515789</guid>
<pubDate>Mon, 23 May 2016 18:18:35 GMT</pubDate>
</item>
<item>
<title> HTTPS, Mixed Content, and the real web... oh my!</title>
<description>&lt;p&gt;We recently fixed something around Mozilla's X-Ray Goggles. A long running problem that caused people headaches and the feeling of lost work, while at the same time doing nothing "wrong", from a technical perspective. This is going to be a story about how modern browsers work, how people use the web, and how those two things... don't always align.&lt;/p&gt;
&lt;h2 id="x-ray-goggles-by-mozilla"&gt;X-Ray Goggles by Mozilla&lt;/h2&gt;
&lt;p&gt;So let's start with X-Ray Goggles: the X-Ray Goggles are &lt;a href="https://goggles.mozilla.org"&gt;a tool&lt;/a&gt; made by &lt;a href="https://mozilla.org"&gt;Mozilla&lt;/a&gt; that lets you "remix" web pages after loading them in your browser. You can go to your favourite place on the web, fire up the goggles (similar to how a professional web developer would open up their dev tools), and then change text, styling, images, and whatever else you might want to change, for as long as you want to change things, and then when you're happy with the result and you want to show your remix to your friends, you can publish that remix so that it has its own URL that you can share.&lt;/p&gt;
&lt;p&gt;However, the X-Ray Goggles use a publishing service that hosts all its content over &lt;code&gt;https&lt;/code&gt;, because we care about secure communication at Mozilla, and using &lt;code&gt;https&lt;/code&gt; is best practice. But in this particular case, it's also kind of bad: large parts of the web still use &lt;code&gt;http&lt;/code&gt;, and even if a website has an &lt;code&gt;https&lt;/code&gt; equivalent, people usually visit the &lt;code&gt;http&lt;/code&gt; version anyway. Unless those websites &lt;em&gt;force&lt;/em&gt; users to the &lt;code&gt;https&lt;/code&gt; version of the site (using a redirect message), then the site they'll be on, and the site they'll be remixing, will use HTTP, and the moment the user publishes their remix with X-Ray Goggles and they get an &lt;code&gt;https&lt;/code&gt; URL back, and then open that URL in their browser....&lt;/p&gt;
&lt;p&gt;well, let's just say "everything looks broken" is not wrong. &lt;/p&gt;
&lt;p&gt;But the reason for this is not because Goggles, or even the browser is doing something wrong - ironically, it's because they're doing something right, and in so doing, what the user wants to do turns out incompatible with what the technology wants them to do. So let's look at what's going on here.&lt;/p&gt;
&lt;h2 id="http-the-basis-upon-which-browsing-is-built"&gt;HTTP, the basis upon which browsing is built&lt;/h2&gt;
&lt;p&gt;If you're a user of the web, no doubt you'll have heard about &lt;code&gt;http&lt;/code&gt; and &lt;code&gt;https&lt;/code&gt;, even if you can't really say what they technically-precisely mean. In simple terms (but without dumbing it down), &lt;a href="https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol"&gt;HTTP&lt;/a&gt; is the language that servers and browsers use to negotiate data transfers. The original intention was for those two to talk about HTML code, so that's where the &lt;code&gt;h&lt;/code&gt; in &lt;code&gt;http&lt;/code&gt; comes from (it stands for "hypertext" in both &lt;code&gt;http&lt;/code&gt; and &lt;code&gt;html&lt;/code&gt;), but we're mostly ignoring that these days, and HTTP is used by browsers and servers to negotiate transmission of all sorts of files - web pages, stylesheets, javascript source code, raw data, music, video, images, you name it.&lt;/p&gt;
&lt;p&gt;However, HTTP is a bit like regular English: you can listen in on it. If you go to a bar and sit yourself with a group of people, you can listen to their conversations. The same goes for HTTP: in order for your browser and the server to talk they rely on a chain of other computers connected to the internet to get messages relayed from one ot the other, and any of those computers can listen in on what the browser and server are saying to each other. In an HTTP setting it gets a little stranger even, because any of those computers could look at what the browser or server are saying, &lt;em&gt;replace what is being said with something else&lt;/em&gt; and then forward that on. And you'll have no way of knowing whether that's what happened. It's literally as if the postal service took a letter you sent, opened it, rewrote it, resealed it, and then sent that on. We trust that they won't, and computers connected to the internet trust that other computers don't mess with the communication, but... they can. And sometimes they do.&lt;/p&gt;
&lt;p&gt;And that's pretty scary, actually. You don't want to have to "trust" that your communication isn't read or tampered with, you want to &lt;em&gt;know&lt;/em&gt; that's the case.  &lt;/p&gt;
&lt;h2 id="what-can-we-do-to-fix-that-"&gt;What can we do to fix that?&lt;/h2&gt;
&lt;p&gt;Well, we can use &lt;a href="https://en.wikipedia.org/wiki/HTTPS"&gt;HTTPS&lt;/a&gt;, or "secure HTTP", instead. Now, I need to be very clear here: the term "secure" in "secure HTTP" refers to secure &lt;em&gt;communication&lt;/em&gt;. Rather than talking "in English", the browser and server agree on a secret language that you could listen to, but you won't know what's being said, and so you can't intercept-and-modify the communication willy-nilly without both parties knowing that their communications are being tampered with. However it does &lt;strong&gt;not&lt;/strong&gt; mean that the data the browser and server agree to receive or send is "safe data". It only means that both parties can be sure that what one of them receives is what the other intended to send. All we can be sure of is that no one will have been able to see what got sent, and that no one modified it somewhere along the way without us knowing.&lt;/p&gt;
&lt;p&gt;However, those are &lt;em&gt;big&lt;/em&gt; certainties, so for this reason the internet's been moving more and more towards preferring HTTPS for everything. But not everyone's using HTTPS yet, and so we run into something called the &lt;a href="https://developer.mozilla.org/en-US/docs/Security/Mixed_content"&gt;"Mixed Content"&lt;/a&gt; issue.&lt;/p&gt;
&lt;h2 id="let-s-look-at-an-example-"&gt;Let's look at an example.&lt;/h2&gt;
&lt;p&gt;Imagine I run a web page, much like this one, and I run it on HTTP because I am not aware of the security issues, and my page relies on some external images, and some JavaScript for easy navigation, and maybe an embedded podcast audio file. All of those things are linked as &lt;code&gt;http://......&lt;/code&gt;, and everything worked fine.&lt;/p&gt;
&lt;p&gt;But then I hear about the problems with HTTP and the privacy and security implications sound horrible! So, to make sure my visitors don't have to worry about whether the page they get from my server is my page, or a modified version of my page, I spring into action, I switch my page over to HTTPS; I get a &lt;a href="https://en.wikipedia.org/wiki/Public_key_certificate"&gt;security certificate&lt;/a&gt;, I set everything on my own server up so that it can "talk" in HTTPS, and done!&lt;/p&gt;
&lt;p&gt;Except immediately after switching, my web page is completely broken! The page itself loads, but none of the images show up, and the JavaScript doesn't seem to be working, and that podcast embed is gone! What happened??&lt;/p&gt;
&lt;p&gt;This is a classic case of &lt;strong&gt;mixed-content blocking&lt;/strong&gt;. My web page is being served on HTTPS, so it's indicating that it wants to make sure everything is secure, but the resources I rely on still use HTTP, and now the browser has a problem: it can't trust those resources, because it can't trust that they won't have been inspected or even modified when it requests them, and because the web page that's asking them to be loaded expressed that it cares about secure communication a great deal, the browser can't just fetch those insecure elements, things &lt;em&gt;might&lt;/em&gt; go wrong, and there's no way to tell!&lt;/p&gt;
&lt;p&gt;So it does the only thing it knows is safe: better safe than sorry, and it flat out refuses to even request them, giving you a warning about "mixed content".&lt;/p&gt;
&lt;p&gt;Normally, that's great. It lets people who run websites know that they're relying on potentially insecure third party content in an undeniably clear way, but it gets a bit tricky in two situations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;third party resources that &lt;em&gt;themselves&lt;/em&gt; require other third party resources, and&lt;/li&gt;
&lt;li&gt;embedding and rehosting&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first is things like your web page using a comment thread service: your web page includes a bit of JavaScript from something like &lt;code&gt;www.WeDoCommentsForYou.com&lt;/code&gt; and then that JavaScript then loads content from that site's comment database, for instance &lt;code&gt;comments.WeDoCommentsForYou.com&lt;/code&gt;. If we have a page that uses HTTPS, running on &lt;code&gt;https://ourpage.org&lt;/code&gt; then we can certainly make sure that we load the comment system from &lt;code&gt;https://www.WeDoCommentsForYou.com&lt;/code&gt;, but we don't control the protocol for the URL that the JavaScript we got back uses. If "WeDoCommentsForYou" wrote their script poorly, and they try to load their comments over &lt;code&gt;http://&lt;/code&gt;, then too bad, the browser will block that. Sure, it's a thing that "WeDoCommentsForYou" should fix, but until they do your users can't comment, and that's super annoying.&lt;/p&gt;
&lt;p&gt;The second issue is kind of like the first, but is about entire web pages. Say you want to embed a page; for instance, you're &lt;a href="https://en.wikipedia.org/wiki/Transclusion"&gt;transcluding&lt;/a&gt; an entire wiki page into another wiki page. If the page you're embedding is &lt;code&gt;http&lt;/code&gt; and the page it's embedded on is &lt;code&gt;https&lt;/code&gt;, too bad, that's not going to work. Or, and that brings us to what I really want to talk about, if you remix a page on &lt;code&gt;http&lt;/code&gt;, with &lt;code&gt;http&lt;/code&gt; resources, and host that remix on a site that uses &lt;code&gt;https&lt;/code&gt;, then that's not going to work either...&lt;/p&gt;
&lt;h2 id="back-to-the-x-ray-goggles"&gt;Back to the X-Ray Goggles&lt;/h2&gt;
&lt;p&gt;And that's the problem we were hitting with X-Ray Goggles, too.&lt;/p&gt;
&lt;p&gt;While the browser is doing the same kind of user protection that it does for any other website, in this particular case it's actually a big problem: if a user remixed an HTTP website, then knowing what we know now, &lt;em&gt;obviously&lt;/em&gt; that's not going to work if we try to view it using HTTPS. But that also means that instead of a cool tool that people can use to start learning about how web pages work "on the inside", the result of which they can share with their friends, they have a tool that lets them look at the insides of a web page and then when they try to share their learning, everything breaks.&lt;/p&gt;
&lt;p&gt;That's not cool.&lt;/p&gt;
&lt;p&gt;And so the solution to this problem is based on first meeting the expectations of people, and then educating them on what those expectations actually mean.   &lt;/p&gt;
&lt;h2 id="give-me-https-unless-i-started-on-http"&gt;Give me HTTPS, unless I started on HTTP&lt;/h2&gt;
&lt;p&gt;There are quite a few solutions to the mixed-content problem, and some are better than others. There are some that are downright not nice to other people on the web (like making a full copy of someone's website and then hosting that on Mozilla's servers. That's not okay), or may open people up exploits (like running a proxy server, which runs on HTTPS and can fetch HTTP resources, then send them on as if they were on HTTPS, effectively lying about the security of the communication), so the solution we settled on is, really, the simplest one:&lt;/p&gt;
&lt;p&gt;If you remix an &lt;code&gt;http://...&lt;/code&gt; website, we will give you a URL that starts with &lt;code&gt;http://&lt;/code&gt;, and if you remix an &lt;code&gt;https://&lt;/code&gt; website, we will give you a URL that starts with &lt;code&gt;https://...&lt;/code&gt;. However, we also want you to understand what's going on with the whole "&lt;code&gt;http&lt;/code&gt; vs &lt;code&gt;https&lt;/code&gt;" thing, so when you visit a remix that starts with &lt;code&gt;http://&lt;/code&gt; the remix notice bar at the top of the page also contains a link to the &lt;code&gt;https://&lt;/code&gt; version --same page, just served using HTTPS instead of HTTP-- so that you can see exactly how bad things get if you can't control which protocol gets used for resources on a page.&lt;/p&gt;
&lt;h2 id="security-vs-usability"&gt;Security vs Usability&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.mozilla.org/en-US/security"&gt;Security is everybody's responsibility&lt;/a&gt;, and explaining the risks on the web that are inherent to the technology we use every day is always worth doing. But that doesn't mean we need to lock everything down so "you can't use it, the end, go home, stop using HTTP". That's not how the real world works.&lt;/p&gt;
&lt;p&gt;So we want you to be able to remix your favourite sites, even if they're HTTP, and have a learning/teaching opportunity there around security. Yes, things will look bad when you try to load an HTTP site on HTTPS, but there's a reason for that, and it's important to talk about it.&lt;/p&gt;
&lt;p&gt;And it's equally important to talk about it without making you lose an hour or more of working on your awesome remix.&lt;/p&gt;
</description>
<category>Mozilla</category>
<category>X-Ray Goggles</category>
<category>http</category>
<category>https</category>
<category>browsers</category>
<category>internet</category>
<link>http://pomax.github.io/#gh-weblog-1462301193409</link>
<guid>http://pomax.github.io/#gh-weblog-1462301193409</guid>
<pubDate>Tue, 03 May 2016 18:46:33 GMT</pubDate>
</item>
<item>
<title>I wrote a  script for Photoshop to export paths as SVG files</title>
<description>&lt;p&gt;Ever drawn some paths in &lt;a href="https://adobe.com"&gt;Adobe&lt;/a&gt;'s &lt;a href="https://adobe.com/photoshop"&gt;Photoshop&lt;/a&gt;, and wanted to export those paths for use in something that &lt;strong&gt;isn't&lt;/strong&gt; Adobe Illustrator? Because that's kind of ... impossible. Photoshop lets you export your working paths specifically for Illustrator, using the &lt;code&gt;.ia&lt;/code&gt; format, but if you want to load it in something else? And heaven forbid, some Open Source package that doesn't even support proprietary formats? You have a problem.&lt;/p&gt;
&lt;p&gt;Or, "had", because now you can export your paths to SVG instead. I needed this functionality for some glyph tracing I'm doing (tracing in Photoshop, but ultimately the glyph needs to end up in &lt;a href="http://fontforge.github.io"&gt;FontForge&lt;/a&gt;) and as it so happens I know that Photoshop &lt;a href="https://www.adobe.com/content/dam/Adobe/en/products/photoshop/pdfs/cs6/Photoshop-CS6-JavaScript-Ref.pdf"&gt;has a Javascript API&lt;/a&gt;. So how hard could it be to create an "export to SVG" option?&lt;/p&gt;
&lt;p&gt;Well... sort of hard. While PS supports JS, the API it has uses a &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/JavaScript#Version_history"&gt;truly ancient&lt;/a&gt;&lt;/em&gt; version of Javascript (v1.3 from what I can tell.. an almost 20 year old flavour), so there were some things that we pretty much rely on these days that simply didn't exist back then. Basic things like &lt;code&gt;Array.forEach&lt;/code&gt; aren't supported, so after writing shims for &lt;code&gt;[forEach](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Array/forEach)&lt;/code&gt;, &lt;code&gt;[map](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/map)&lt;/code&gt; and &lt;code&gt;Object.keys&lt;/code&gt;, I was able to write a script that lets you simply call &lt;em&gt;file&lt;/em&gt; → &lt;em&gt;scripts&lt;/em&gt; → &lt;em&gt;browse&lt;/em&gt; and then run it, which immediately generates the SVG code for all your paths as a single SVG document and copies the code for it into your clipboard.&lt;/p&gt;
&lt;p&gt;Internally, Photoshop stores paths as "multiple paths per document", with "multiple sub-paths per path", and each sub-path consists of an array of points. These points can either be corner points or "smooth" points, meaning they're Bezier curve points, and that's all the information you get.&lt;/p&gt;
&lt;p&gt;Of course, while this is a normal way to encode data, &lt;a href="https://www.w3.org/TR/SVG/paths.html#PathData"&gt;SVG path instructions&lt;/a&gt; are of the form "Line to x/y" or "Cubic curve over c1 and c1 to x/y", so in order to generate the correct path instructions, the code needs to walk over each sub-path using a two point window, and check whether it's generating code for going from corner to corner, corner to curve, curve to corner, or curve to curve; each of those four has a different way to unpack the information contained by the two points into a single SVG pathing instruction.&lt;/p&gt;
&lt;p&gt;But, with that done, the result is a Photoshop script that with just a few clicks drops your entire working path collection into an SVG document in your clipboard. If that sounds useful to you, head on over to &lt;a href="https://github.com/Pomax/photoshop-paths-to-SVG"&gt;https://github.com/Pomax/photoshop-paths-to-SVG&lt;/a&gt; and grab the script for yourself.&lt;/p&gt;
&lt;p&gt;And  of course it's open source, so let me know if you find it useful or have ideas on how to improve it!&lt;/p&gt;
</description>
<category>Photoshop</category>
<category>Javascript</category>
<category>SVG</category>
<category>Paths</category>
<link>http://pomax.github.io/#gh-weblog-1457651549619</link>
<guid>http://pomax.github.io/#gh-weblog-1457651549619</guid>
<pubDate>Thu, 10 Mar 2016 23:12:29 GMT</pubDate>
</item>
<item>
<title>Github changes to icon SVG from fonts, for no clear reason</title>
<description>&lt;p&gt;In a &lt;a href="https://github.com/blog/2112-delivering-octicons-with-svg"&gt;recent post on their blog&lt;/a&gt;, &lt;a href="github.com"&gt;github&lt;/a&gt; announced that  they've moved away from icon fonts in favour of SVG. In itself perfectly fine, use whatever works best, and if fonts aren't working for you, switch; but their explanation of what was wrong makes several kinds of no sense.&lt;/p&gt;
&lt;p&gt;This is going to be a bit of a rant, it's going to conclude with "should they switch? absolutely! yay github" but the rest of the text is about how their blog post doesn't feel like it actually contains any proper rationale for the move. You can stop reading if you only wanted to know my concluding thoughts. As far as I'm concerned, they did the right thing, for the wrong reasons.&lt;/p&gt;
&lt;p&gt;Also, I've &lt;a href="http://stackoverflow.com/a/30821089/740553"&gt;talked about this on Stackoverflow&lt;/a&gt; before, so if you ever ran into that post, the following analysis should not come as anything even close to a surprise to you.&lt;/p&gt;
&lt;h2 id="showing-the-problem"&gt;Showing the problem&lt;/h2&gt;
&lt;p&gt;The post starts with images that show a "badly blurred font icon" vs "a nicely crisp image icon", except what it really shows are two equally blurry, terrible images. Neither is particularly better than the other at the scale shown; they're differently blurry, but they're both terrible, and using this as a highlight of the problem shows that (unsurprisingly) a vector approach to icons is a weird solution to a problem predominantly &lt;a href="http://rastertragedy.com"&gt;caused by small-scale rendering of graphics designed using large scale units&lt;/a&gt;. They're both bad, so the rest of the post better be good at explaining why you want the lesser of two "evils" instead of finding a third "good".&lt;/p&gt;
&lt;h2 id="icon-font-rendering-issues"&gt;Icon font rendering issues&lt;/h2&gt;
&lt;p&gt;The first section is 100% true. Github notes that "Icon fonts have always been a hack", and they have. Unicode these days comes with dingbats/windings, symbol sets, and emoji, but none of those things are true icons. Icon fonts will typically either inappropriately use "real letter" code points for icons ("you use class &lt;code&gt;.left-arrow&lt;/code&gt; but it renders because it asks the letter &lt;code&gt;a&lt;/code&gt; from the font, which happens to be an icon) or they use PUA (the Private Use Area of fonts) and some kind of substitution ruleset for finding icons in that area (because you can't really type PUA letters on your keyboard).&lt;/p&gt;
&lt;p&gt;So far so good. Except that's not a reason to switch, that's merely stating the facts. Fonts for icons are hacks. Do they get the job done? Certainly. This does not count towards the "reasons to switch" yet.&lt;/p&gt;
&lt;h2 id="page-rendering-improvements"&gt;Page rendering improvements&lt;/h2&gt;
&lt;p&gt;There was an assumption that SVG would improve page performance. However, as they note later, &lt;a href="https://cloud.githubusercontent.com/assets/54012/13176951/eedb1330-d6e3-11e5-8dfb-99932ff7ee25.png"&gt;there's no clear evidence of that&lt;/a&gt;. To be clear: this is completely the right thing to do: you have two possible options, you test both, and then use metrics to determine which one is better. But if there is no real difference then it cannot count as reason to switch. It is in fact a reason to tell others "if this is why you're thinking of switching, it doesn't matter, here's our findings: ..."&lt;/p&gt;
&lt;p&gt;So far, we have no justifiable reason to switch yet (other than just "let's do this").&lt;/p&gt;
&lt;h2 id="accessibility"&gt;Accessibility&lt;/h2&gt;
&lt;p&gt;This one's interesting. "some people use their own fonts, so our fonts don't render" may be a problem. Github has &lt;em&gt;a lot&lt;/em&gt; of users, but "people using their own fonts" typically also means those people do so by choice. If overriding the icon font is adverse, they presumably have the power to each fix that, but chose not to. That's not Github's responsibility to fix, but it can certainly be something Github can fix "as one party" instead of every user who's affected fixing it "as a million parties". It's not an argument to switch from fonts to SVG in general, but it's an argument for Github, as a trusted and liked millions-of-users-daily service, to lend a helping hand.&lt;/p&gt;
&lt;p&gt;The second argument concerns screen readers, but glosses over the nature of an icon in a UI. Icons are non-semantic, they're decoration around the actual thing. The hypothetical ability to add readable &lt;code&gt;alt&lt;/code&gt; texts because the icons are now true images does not apply: if your icon needs an alt-text read out to the user in order for them to understand your UI, that icon is not an icon, it's a genuine semantic element and you should not be using an icon to hide what it is. In fact, you confuse &lt;em&gt;everyone&lt;/em&gt; with that kind of icon use, not just people who need, or are better served by using, screen readers.&lt;/p&gt;
&lt;h2 id="properly-sized-glyphs"&gt;Properly sized glyphs&lt;/h2&gt;
&lt;p&gt;This one's a bit odd, because it both makes sense and absolutely not. I'm paraphrasing, but this section has a rational along the lines of "vector fonts render horribly at small point sizes", which is &lt;a href="http://rastertragedy.com"&gt;absolutely true&lt;/a&gt;, "as opposed to SVG, which renders pixel-perfect", which is absolutely false.  Both are vector graphics, and both suffer from the exact same problem scaled down. However, fonts can come with instructions on how to correct outlines, and SVG cannot. Unfortunately the choice between &lt;code&gt;ttf&lt;/code&gt; and &lt;code&gt;otf&lt;/code&gt; matters &lt;em&gt;a lot&lt;/em&gt; here: &lt;code&gt;ttf&lt;/code&gt; uses true instructions, and &lt;code&gt;otf&lt;/code&gt; uses hints. If you try to use an &lt;code&gt;otf&lt;/code&gt; icon font with a render engine that doesn't have a good "what should I do with these hints" implementation, your icons will look terrible. Turns out that's the state of technology right now. So that's an argument to use &lt;code&gt;ttf&lt;/code&gt; rather than &lt;code&gt;otf&lt;/code&gt;, rather than exchanging one vector format for another.&lt;/p&gt;
&lt;p&gt;The blog post does talk about SVG not getting subpixel-hinted, which "fixes" things, but if that's the case you've found a bug in the text render pipeline: file that so it can be fixed. In fact as &lt;a href="https://twitter.com/Keyamoon/status/703530343169560576"&gt;Keyamoon points out&lt;/a&gt;, this is not the case in Firefox, and &lt;em&gt;wasn't&lt;/em&gt; the case in previous versions of Chrome. As arguably the most important playing in Open Source Software at the moment, I would expect Github to help get this fixed instead of walking away from a problem like that.&lt;/p&gt;
&lt;p&gt;But why go from one vector format to another? Vector graphics are always worse than bitmaps if you want pixel-perfect icons, so: why not  keep using fonts, with bitmaps for the "it scales poorly" cases? I know what you're thinking: "but modern fonts are vector fonts!" Not really, no. Modern fonts can do &lt;em&gt;a lot&lt;/em&gt; of things, and one of those things is that they can contain pixel-perfect bitmaps to be used when a sequence of glyphs needs to be shaped at &lt;em&gt;specific&lt;/em&gt; point sizes, for pixel-perfect rendering (using the EBDT, Embedded Bitmap Data Table). If your icon font of choice doesn't come with those, then that's an argument for improving things on the font side, so that everyone who uses that font gets a better deal out of it.&lt;/p&gt;
&lt;p&gt;Of course, in the mean time you might consider changing over to SVG because you want to solve the problem "now", and I like Github and assume they looked and couldn't find one. But as phrased, this section didn't actually give a good rationale for switching to SVG so much as highlighted a misunderstanding on how modern fonts work and showing that assumptions were made about what the problem was, leading to an action that doesn't &lt;em&gt;solve&lt;/em&gt; the problem: to someone familiar with the inner workings of fonts, this section reads as a symptom description, not a problem description, and fixing the symptom does not solve the underlying problem. &lt;em&gt;Any vector graphic&lt;/em&gt; will render terribly at low point sizes, trading one for another just &lt;a href="https://en.wikipedia.org/wiki/Buck_passing"&gt;passes the buck&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="ease-of-authoring"&gt;Ease of authoring&lt;/h2&gt;
&lt;p&gt;No two ways about it: if you want to control your icons, SVG is a hell of a lot easier than fonts. Go with SVG! It's the right choice! &lt;/p&gt;
&lt;h2 id="we-can-animate-them"&gt;We can animate them&lt;/h2&gt;
&lt;p&gt;Get out. If you're arguing accessibility as part of your rationale, don't even go here.&lt;/p&gt;
&lt;h1 id="should-github-switch-"&gt;Should Github switch?&lt;/h1&gt;
&lt;p&gt;Of course they should, it's their site. If they feel better with an SVG solution compared to fonts, then that's all the rationale they need. But then it would be nice if their blog post acknowledged that the only objective justification that was described, performance, turned out to not be significantly impacted by the switch, and that they've gone through with the switch mostly because it's easier to work with from a continuous update perspective, and also quite likely because if you've done all that work already to test deploying SVG instead of fonts, why not flip the switch.&lt;/p&gt;
&lt;p&gt;I'm sure we'd understand that, too.&lt;/p&gt;
</description>
<category>Github</category>
<category>Fonts</category>
<category>SVG</category>
<category>Icons</category>
<link>http://pomax.github.io/#gh-weblog-1456332505750</link>
<guid>http://pomax.github.io/#gh-weblog-1456332505750</guid>
<pubDate>Wed, 24 Feb 2016 16:48:25 GMT</pubDate>
</item>
<item>
<title>You probably need Chocolate Bread Pudding in your life.</title>
<description>&lt;p&gt;I made an &lt;a href="http://imgur.com/gallery/ObTlB"&gt;imgur image album&lt;/a&gt; as an illustrated guide to making chocolate bread pudding - or at least, the kind I make these days (less death-by-chocolate, more tasty-not-too-sweet brownie alternative). If there's requests I'll turn that into a post on this blog, but the imgur link should be pretty stable so for now I'll leave it at that.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/GDjqsU0.jpg" alt=""&gt;&lt;/p&gt;
</description>
<category>Cooking</category>
<category>Baking</category>
<category>Chocolate</category>
<link>http://pomax.github.io/#gh-weblog-1455215020166</link>
<guid>http://pomax.github.io/#gh-weblog-1455215020166</guid>
<pubDate>Thu, 11 Feb 2016 18:23:40 GMT</pubDate>
</item>
<item>
<title> A blog post about kitchen knives</title>
<description>&lt;p&gt;I've been promising to write this blog post for a while after joking about how the fair number of knives I use in the kitchen would probably fill an entire blog post, but for reasons that have been lost to time never got to it. That changes now.&lt;/p&gt;
&lt;p&gt;I like working in the kitchen, I started as a kid, kept it up as a student, and had the pleasure to have access to a good kitchen with gas range after moving to Canada from the Netherlands. When it came time to buy our own house, we specifically bought one with a large kitchen and gas range, and I still get my cook on on a daily -if not more- basis.&lt;/p&gt;
&lt;p&gt;Now, cookery requires ingredients, and ingredients quite often require cutting, so you buy a cheap knife set, you use two or three of them frequently, and you can get by quite well. The price of a knife set does not determine how well the knives will work, and some of the more expensive ones (I'm looking at you, Global) are absolutely terrible to have to work with on a daily basis. Better to get a cheap set that cuts well and have a friend grind down the handle a bit where the metal bits and wooden bits don't quite line up, than to pay ten times as much or more for great quality that's horrible to work with.&lt;/p&gt;
&lt;p&gt;Now, two or three knives is literally all you need. If I were caught in a situation where I had to prepare food for 40 people and all I had was a chef knife and a paring knife, I'd be fine. But they do make specialty knives, and having them makes certain tasks either more fun, or much easier, so I've picked up a few knives over the years and of those, this blog post will highlight the ones I use regularly, meaning either daily, or at least several times a month. Except for two, but we'll get to them.&lt;/p&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/knives/knives-01.jpg" alt=""&gt;&lt;/p&gt;
&lt;p&gt;Let's talk about this collection, shall we?&lt;/p&gt;
&lt;h2 id="the-chef-knife"&gt;The chef knife&lt;/h2&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/knives/knives-02.jpg" alt=""&gt;&lt;/p&gt;
&lt;p&gt;The chef knife is, as far as I'm concerned, the absolute minimum necessary to "do everything". It can slice, it can dice (well, with your help), it can chop, it can pare (while a little inconveniently, it absolutely can), it can fillet, it can crush, as far as I'm concerned it is the only knife you &lt;strong&gt;need&lt;/strong&gt; in a kitchen. Although that doesn't mean you should use it for all those things because you'll probably have at least one or two knives that can do one or more of those things better. &lt;/p&gt;
&lt;p&gt;I have a few chef knives, but these are the two I use on a daily basis. The top is essentially my backup for when more than one person's in the kitchen, if it's just me I'll typically use... well, perhaps the level of wear will give it away: I use the knife with the broken tip. It's from a $30 knife set I bought over 10 years ago, and is as sharp as any $200 knife. In terms of actual weight, it's fairly heavy, but it's extremely well balanced so it feels like it has very little true weight to it, and it dances around in your hand if you want it to. Yes, that's the level of satisfaction a nice kitchen knife can give &lt;/p&gt;
&lt;p&gt;I use these for big cuts, slicing, as well as chopping up things. Bog standard use really.&lt;/p&gt;
&lt;h2 id="the-utility-knife"&gt;The utility knife&lt;/h2&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/knives/knives-03.jpg" alt=""&gt;&lt;/p&gt;
&lt;p&gt;The utility knife is for smaller jobs. The blade's too short for most things a chef knife is good for, but it's great for incisions and stripping (both of those apply to vegetables as much as to meats).&lt;/p&gt;
&lt;h2 id="the-paring-knife"&gt;The paring knife&lt;/h2&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/knives/knives-04.jpg" alt=""&gt;&lt;/p&gt;
&lt;p&gt;A paring knife is the scalpel of the basic set. It's not good for any kind of slicing, but it's great for precision cuts, and cutting things away from each other with minimum spill-over, or creating small cutouts (like taking a stem out of a bell pepper. No need to waste tasty vegetable goodness by cutting off the entire top). &lt;/p&gt;
&lt;h2 id="a-nice-sharp-cleaver"&gt;A nice sharp cleaver&lt;/h2&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/knives/knives-05.jpg" alt=""&gt;&lt;/p&gt;
&lt;p&gt;While this looks like a super heavy thing, and it's certainly heavier than a regular knife, this old Solingen cleaver is essentially the same as a thick Chinese cleaver. It has a sharp edge, and can be used for cutting, chopping, and because of the geometry of the blade near the handle, shucking oysters or other shellfish. And, of course, it's a cleaver so you can use it to cleave, although it's mostly for cleaving &lt;em&gt;soft&lt;/em&gt; things.&lt;/p&gt;
&lt;p&gt;Simple rule: things with sharp edges are meant for cutting, not hitting. The value in a cleaver is its size and solidity; do the initial cut, drive the cleaver in, then drive it through. The softer the thing you're cleaving, the more you can "chop" rather than drive, but this is not TV, and you don't wield a regular cleaver like it's a hatchet.   &lt;/p&gt;
&lt;p&gt;For instance: chunking up Chinese barbecue pork: yes. cleaving through a lamb shoulder: no. Splitting a squash: absolutely. Chopping a chicken thigh in half: ... kind of not, no. I mean, you can if you absolutely have to, but really you want the next item on this list instead.&lt;/p&gt;
&lt;h2 id="a-nice-not-really-sharp-bone-cleaver"&gt;A nice not-really-sharp bone cleaver&lt;/h2&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/knives/knives-06.jpg" alt=""&gt;&lt;/p&gt;
&lt;p&gt;This is a real cleaver in the sense of "what people think of when you say cleaver". It basically a kitchen axe with a relatively dull edge. You don't cut things with this (you plain old can't) but you use it as a splitter to get through bones. Or, sometimes, things that have frozen up pretty solid. &lt;/p&gt;
&lt;p&gt;Unlike a sharp cleaver, bone cleavers &lt;em&gt;are&lt;/em&gt; wielded like hatchets: it's pretty much what they are. I obviously don't use this every day, it's a specialty knife (if we can really still call it a knife) but this is the only tool I want to use when I do need to use it.&lt;/p&gt;
&lt;h2 id="a-yanagi-sashimi-knife"&gt;A yanagi (sashimi) knife&lt;/h2&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/knives/knives-07.jpg" alt=""&gt;&lt;/p&gt;
&lt;p&gt;"Japanese knives!", aye. The yanagi knife is a sashimi knife, meaning it's intended for single cut, precision-slicing. It might look a bit long, but honestly I'm looking into replacing it with a longer version: sashimi is raw, which means nothing will have been done to the cut to hide imprecisions or imperfections, and the human tongue is an amazing organ: it will detect rough cuts. The longer the knife, the longer you can make a single, constant pressure cut, which means the cut surface will be absolutely smooth. Unlike a chef knife, where you're often cutting away from yourself, you cut towards yourself with a sashimi knife. Ever seen or bowed a violin? It's pretty much like that. Very little pressure, let the knife do the cutting as you pull it in.&lt;/p&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/knives/knives-08.jpg" alt=""&gt;&lt;/p&gt;
&lt;p&gt;Unlike the previous knives, this is a single sided knife: it's perfectly flat on the left side, and ground to an extremely sharp edge on the other. At least for sashimi, this means you don't disturb half the cut by "pushing it out of the way", which a "normal" double sided knife will do.&lt;/p&gt;
&lt;h2 id="a-deba-fish-knife"&gt;A deba fish knife&lt;/h2&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/knives/knives-09.jpg" alt=""&gt;&lt;/p&gt;
&lt;p&gt;Fish knife! This is a thick, heavy knife with a single sided edge. It's extremely sharp, and great for deboning fish. If you have an whole salmon (and I live in Vancouver, a whole salmon can be yours for about $10) then this knife is all you need to turn it into two beautiful full length fillets.&lt;/p&gt;
&lt;h2 id="an-usuba-vegetable-knife"&gt;An usuba vegetable knife&lt;/h2&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/knives/knives-10.jpg" alt=""&gt;&lt;/p&gt;
&lt;p&gt;Finally, the Japanese vegetable knife. Again, single sided edge, and it's incredibly sharp. It sits at the intersection of chef knife and cleaver, and is great for splitting not-too-hard vegetables, as well as chopping up vegetables if presentation is going to matter, like uniformly hair-fine julienne, peeling a cucumber to get the entire skin as a single sheet, getting wafer thin slices of vegetable, et cetera.&lt;/p&gt;
&lt;p&gt;I love this knife to bits. &lt;/p&gt;
&lt;h2 id="the-ol-bread-knife"&gt;The ol' bread knife&lt;/h2&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/knives/knives-15.jpg" alt=""&gt;&lt;/p&gt;
&lt;p&gt;On to pure utility: the bread knife. Some people don't have one, some do but never use it, in our household this thing is pretty much used on a daily basis. It's not actually all that good at slicing bread, but it's a great knife for cutting frozen items, as well as "opening up" a bread for slicing, which basically means cutting it in half so that the next knife on the list can be used...&lt;/p&gt;
&lt;h2 id="a-bread-saw"&gt;A bread saw&lt;/h2&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/knives/knives-11.jpg" alt=""&gt;&lt;/p&gt;
&lt;p&gt;That's right, it's a saw. On a handle. For slicing bread. After you open up a bread with the regular breadknife, all the other slices are simply "put this on bread so that the wood touches the side and the saw touches the top. Saw off slice". It is perfect.&lt;/p&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/knives/knives-12.jpg" alt=""&gt;&lt;/p&gt;
&lt;p&gt;It's also literally just a saw blade. I stopped counting the number of times I accidentally cut myself on this thing. Most knives don't draw blood just by being tapped. This thing? No mercy.&lt;/p&gt;
&lt;h2 id="keeping-knives-sharp"&gt;Keeping knives sharp&lt;/h2&gt;
&lt;p&gt;And let's not forget the things we need to make sure knives stay usable. Knife edges wear off over time and with use, and hard as metal is, a very sharp blade is thin, and can easily dent or chip. To deal with this, there's three tools in my kitchen for making sure the knives stay usable. &lt;/p&gt;
&lt;h3 id="a-good-steel"&gt;a good steel&lt;/h3&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/knives/knives-14.jpg" alt=""&gt;&lt;/p&gt;
&lt;p&gt;First off, a good steel. Steels are not for sharpening a knife, they're for making sure any dents in the cutting edge are smoothed out by "steeling" it: rubbing the edge along the steel, doing both sides to make sure dents in either direction and smoothed back in line with the rest of the edge. Do this when you're done with your knife after cutting something that offered a bit of resistance and your knife'll last a bit longer. &lt;/p&gt;
&lt;h3 id="grind-stone-s-and-carbide-shaver"&gt;grind stone(s) and carbide shaver&lt;/h3&gt;
&lt;p&gt;&lt;img src="/gh-weblog-2/images/knives/knives-13.jpg" alt=""&gt;&lt;/p&gt;
&lt;p&gt;Of course, steeling isn't sharpening, and your knife &lt;em&gt;will&lt;/em&gt; lose its edge eventually. When that happens, it's time to whip out the grinding stones. I have a combination 400 grit/2000 grit whetstone, and badly dulled knives first get a treatment at 400 before moving on to 2000.&lt;/p&gt;
&lt;p&gt;Finally there's that little "speedy sharp" thing: that's a carbide steel strip on a handle. A word of warning: this is &lt;em&gt;not&lt;/em&gt; a sharpener. This is a shaver, and does what that word implies: the carbide steel strip is much, much stronger than the metal used to make  your knife, and running this across the cutting edge literally shaves off some of the metal. It's a very fast way to give a knife a quick edge, but you've just reduced your knife width by far more than you would if you used a sharpening stone. Use this as your only method of sharpening and you're literally going to just run out of knife. That said, I use this to quickly strip a knife of whatever oxidation or environmental coating it may have picked up when I bake baguettes, where getting a good scoring across the loaves before the bread goes into the oven is far more important to me than wearing out a knife. Just stroke the edge with minimal force a few times and you're good to go. As long as you remember to also give the knife a quick rinse because again: you're shaving it. There will be residue.   &lt;/p&gt;
&lt;h1 id="and-that-s-it-"&gt;And that's it.&lt;/h1&gt;
&lt;p&gt;That's all the knives I use regularly enough to feel they warrant mention - Notably missing are my Chinese cleaves (#1, #2, and #5) that I basically stopped using because between the chef knives and Japanese knives, there's nothing they're actually better at, and while nifty, things like grapefruit knives are so special purpose that if you need them, you know why, and if you don't know why, you won't need them.&lt;/p&gt;
</description>
<category>Kitchen</category>
<category>Knives</category>
<category>Knife</category>
<link>http://pomax.github.io/#gh-weblog-1454267122337</link>
<guid>http://pomax.github.io/#gh-weblog-1454267122337</guid>
<pubDate>Sun, 31 Jan 2016 19:05:22 GMT</pubDate>
</item>
<item>
<title> "git init" your way to peace of mind.</title>
<description>&lt;p&gt;This is going to be a short post, but it's one that can make the difference between you being happy, and crying with your head in your hands because you absolutely ruined a file and hit save.  I'm basically going to tell you to use version control for stuff that matters, but rather than letting you figure out how to do that, I'm simply going to straight up tell you.&lt;/p&gt;
&lt;h2 id="what-do-i-need-version-control-for-i-don-t-program"&gt;What do I need version control for, I don't program&lt;/h2&gt;
&lt;p&gt;Who cares about programming? Do you write text on your computer? Maybe Word docs? A personal web page? Blog entries as .txt files you'll copy-paste into wordpress later? It really doesn't matter: the real question is "do you have folders/directories with data in them that you change every now and then, and that you might mess up because you can accidentally hit save and overwrite them, or accidentally hit delete and lose them forever?".&lt;/p&gt;
&lt;p&gt;Because the answer is pretty much guaranteed "yes", and you could try to be diligent about never hitting save or delete, but you're human, and you're going to mess up. Maybe not today, maybe not next week, but chances are pretty much 100% at some point, you'll mess up.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;so use git&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="what-do-i-need-git-for-i-don-t-want-to-put-my-stuff-online"&gt;What do I need "git" for, I don't want to put my stuff online&lt;/h2&gt;
&lt;p&gt;Yeah, common misconception. "git" has nothing to do with "putting stuff online", but has everything to do with "being version control software". In order to be successful version control software, you need to be able to do remote syncing because the vast majority of version controlled data is collaborative data, but it's 100% not necessary for the version control part, so here's what you do:&lt;/p&gt;
&lt;h2 id="safeguard-your-data-from-you-"&gt;Safeguard your data from &lt;em&gt;you&lt;/em&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;install &lt;a href="https://git-scm.com/downloads"&gt;git&lt;/a&gt;. It's a few clicks, just do it.&lt;/li&gt;
&lt;li&gt;For any dir/folder that holds a project you do work on, run "git init". If you are familiar with the command line, it's literally just &lt;code&gt;$&amp;gt; cd your/project/dir&lt;/code&gt; followed by &lt;code&gt;$&amp;gt; git init&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Awesome, you now have version control running for that dir/folder.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="okay-what-s-next-"&gt;Okay, what's next?&lt;/h2&gt;
&lt;p&gt;Every time you make changes to your files, even if you're not done but you made some headway on them and feel like now would be a good time to take a snapshot: take a snapshot!&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;$&amp;gt; git add -A&lt;/code&gt; (tell git to look at &lt;em&gt;all&lt;/em&gt; the changes that were made since the last snapshot)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$&amp;gt; git commit -m "give your snapshot a name and/or description here"&lt;/code&gt; (include the quotes)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Done. No syncing with the internet, no complicated workflow stuff, just &lt;code&gt;git add&lt;/code&gt; to add all the changes, and then &lt;code&gt;git commit&lt;/code&gt; to commit the snapshot to the version history.&lt;/p&gt;
&lt;h2 id="okay-i-ve-done-that-and-then-a-few-minutes-later-i-ruined-all-my-files-what-now-"&gt;Okay, I've done that! And then a few minutes later I ruined all my files! What now!?!?!&lt;/h2&gt;
&lt;p&gt;Say you accidentally deleted the files you needed from your project dir, and then to make things worse you also saved empty files on top of that because your text editor spazzed out! Oh no! If you didn't have &lt;code&gt;git&lt;/code&gt; managing your version control, you'd be completely screwed!&lt;/p&gt;
&lt;p&gt;But you have git managing your snapshot history, and as long as you still have its &lt;code&gt;.git&lt;/code&gt; dir, what you just did is effectively insignificant: just tell git to restore your last snapshot:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$&amp;gt; git reset --hard&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;And done. All your files are back, in the state they were in during the last snapshot. Sure, you may have lost a few minutes of work, but between that and "I lost everything", I'm pretty sure we'd both pick the few minutes of work lost option.&lt;/p&gt;
&lt;h2 id="it-worked-but-then-i-messed-up-again-and-this-time-i-also-deleted-my-git-directory-"&gt;It worked! But then I messed up &lt;em&gt;again&lt;/em&gt; and this time I also deleted my &lt;code&gt;.git&lt;/code&gt; directory!!&lt;/h2&gt;
&lt;p&gt;That's the only way to make sure &lt;code&gt;git&lt;/code&gt; can't help you anymore. If your workflow is such that you &lt;em&gt;can&lt;/em&gt; delete entire directories, then really you need to get yourself some remote syncing/backup solution. That's kind of out of scope for this short post on "version control your stuff already, you are smart enough to understand the need for it", but you could look at things like &lt;a href="https://www.dropbox.com/"&gt;Dropbox&lt;/a&gt; and/or &lt;a href="https://www.backblaze.com"&gt;backblaze&lt;/a&gt;. If your work is that important, why are you not paying the $5/mo to properly protect &lt;em&gt;all the data on your computer&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;Also, if you have any kind of user account control, make sure to set the &lt;code&gt;.git&lt;/code&gt; directory (but not its content) to read-only, so that even if you accidentally try to delete it, your OS will go "hey you can't actually delete this dir, okay? you're going to have to unlock it before I can do that for you". Depending on your OS the way you do this differs, but again instead of telling you to look up how to do that, I'm just going to tell you.&lt;/p&gt;
&lt;p&gt;On Linux, Unix, and OSX, you can use the terminal to issue &lt;code&gt;$&amp;gt;chmod 0444 .git&lt;/code&gt; in your project's directory, which sets the permissions for the ".git" dir (but not its content) to read-only. Unless you're deleting things with &lt;code&gt;rm&lt;/code&gt;, that dir will be relatively safe now.&lt;/p&gt;
&lt;p&gt;On windows, use any filemanager and right click the dir's folder, and then make sure to check the "read-only" box. On windows deleting the .git dir is usually less of a problem because the dir's marked as "hidden". You'll see it if you're a power user who unhides system and hidden dirs/files, but in normal use you won't see it, and thus won't accidentally delete it.&lt;/p&gt;
&lt;h2 id="i-didn-t-actually-mess-anything-up-but-i-d-like-to-look-at-an-old-snapshot-"&gt;I didn't actually mess anything up, but I'd like to look at an old snapshot...&lt;/h2&gt;
&lt;p&gt;Yeah that's a thing you can do. You can scroll through your snapshot history by using &lt;code&gt;$&amp;gt; git log&lt;/code&gt;, which will tell show your the name/descriptions for each snapshot (which is why you want to at least use sensible naming/descriptions. A single dot is fast to write, but useless if you ever need to go through your history), tied to a git "commit hash", which is a long hexadecimal string that uniquely identifies that snapshot for the git application. Say I need last week's version of a novel I'm writing, because I rewrote half a chapter but I remember there was a paragraph that had super sweet phrasing and I want it back:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;$&amp;gt; git add -A&lt;/code&gt; because I don't want to lose any work&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$&amp;gt; git commit -m "midday work, somedate-2016"&lt;/code&gt; so that git has a snapshot for where I am right now.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$&amp;gt; git log&lt;/code&gt;, to see my history&lt;/li&gt;
&lt;li&gt;Notice that "end of day, Monday, week 2, 2016" has commit "dbcaf87689facb6bcb6f9eb786fs9".&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$&amp;gt;git checkout dbcaf87689facb6bcb6f9eb786fs9&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Boom, git changes everything to match what they were at the time I took that snapshot. I can copy the bits I need onto my desktop, then make sure to put me back on where I just was&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$&amp;gt; git reset master&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This changes all my files to where I just was again, and I can go on with my life. So handy!&lt;/p&gt;
&lt;h2 id="what-else-can-i-do-"&gt;What else can I do?&lt;/h2&gt;
&lt;p&gt;Quite a lot, but all of that's covered by &lt;a href="https://git-scm.com/book/en/v2"&gt;the git book&lt;/a&gt;, and a lot of it is mostly irrelevant unless you also want to do remote syncing of your data. Still, good read if you really care about your data.&lt;/p&gt;
&lt;h2 id="what-if-the-command-line-is-too-unfamiliar-to-me-"&gt;What if the command line is too unfamiliar to me?&lt;/h2&gt;
&lt;p&gt;You can use the &lt;code&gt;git-gui&lt;/code&gt; command. This will ask you (if you "just run it") whether to create a new repository, or manage an existing one. I'm not going to explain how to use that, because  I never use it, but it can do all the things this post talked about.&lt;/p&gt;
&lt;h1 id="that-s-it-we-re-done-"&gt;That's it, we're done.&lt;/h1&gt;
&lt;p&gt;Go use version control. You &lt;em&gt;litearlly&lt;/em&gt; have no excuse not to. And if you have any questions or comments, you can either leave those over on github (see the link below this post) or just tweet at me &lt;a href="http://twitter.com/TheRealPomax"&gt;@TheRealPomax&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Version control your files, because you shouldn't be able to be your own enemy.&lt;/p&gt;
</description>
<category>Git</category>
<category>Version Control</category>
<link>http://pomax.github.io/#gh-weblog-1453231679399</link>
<guid>http://pomax.github.io/#gh-weblog-1453231679399</guid>
<pubDate>Tue, 19 Jan 2016 19:27:59 GMT</pubDate>
</item>
<item>
<title> React with LaTeX, without needing client-side Mathjax</title>
<description>&lt;p&gt;It's no secret that I like working with &lt;a href="https://facebook.github.io/react"&gt;React&lt;/a&gt;, and that I like &lt;a href="https://en.wikipedia.org/wiki/LaTeX"&gt;LaTeX&lt;/a&gt; for my maths, and that I like &lt;a href="https://mathjax.org"&gt;MathJax&lt;/a&gt; for my LaTeX on the web. What I don't like, though, is having to wait 20 seconds for a long article with lots of maths to load client-side, constantly changing the page dimensions as it does so. Especially if you're loading it mid-page and then MathJax kicks in and all the content keeps being pushed down. And again. And again, etc. etc.&lt;/p&gt;
&lt;p&gt;It's annoying for the user, and as the guy who runs a &lt;a href="http://pomax.github.io/bezierinfo"&gt;big article on Bézier curves&lt;/a&gt;, with lots of maths, it's annoying to know I'm responsible for a bad experience. So, I'm rewriting that article so that it's easier to maintain, and loads a million times faster. One of the things that involves is taking MathJax out of the client-side experience, which means during LaTeX conversion server-side. Or really, "offline", because it should be a "generate once, then cache and use" instead of "having the server generate it all the time".&lt;/p&gt;
&lt;h2 id="react-and-webpack-and-babel-"&gt;React and Webpack (and Babel)&lt;/h2&gt;
&lt;p&gt;First off, a React codebase needs bundling, and generally also some &lt;a href="https://facebook.github.io/react/docs/jsx-in-depth.html"&gt;JSX&lt;/a&gt; transforming and &lt;a href="https://babeljs.io/docs/learn-es2015"&gt;ES2015+&lt;/a&gt; conversion, so the basis for my rewrite was a fairly simple dir layout with a fairly simple &lt;a href="https://webpack.github.io"&gt;Webpack&lt;/a&gt; config:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./
 |-&amp;lt;components&amp;gt;
 |-&amp;lt;images&amp;gt;
 |-&amp;lt;lib&amp;gt; 
 |-&amp;lt;stylesheets&amp;gt;
 | package.json
 | webpack.config.js
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With the Webpack config doing the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var webpack = require('webpack');

// Hot Reload server when we're in dev mode, otherwise build it the normal way.
var entry = ['./components/App.jsx'];
if(!process.argv.indexOf("--prod")) { entry.push('webpack/hot/dev-server'); }

module.exports = {
  entry:  entry,
  output: {
    path: __dirname,
    filename: 'article.js'
  },
  module: {
    loaders: [
      { test: /\.(png|gif)$/, loader: "file?name=images/packed/[hash].[ext]" },
      { test: /\.less$/, loader: "style!css!less" },
      { test: /.jsx?$/, include: /components/, loaders: ['babel-loader']
      }
    ]
  },
};
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This is a pretty standard setup, with webpack's &lt;a href="https://webpack.github.io/docs/hot-module-replacement-with-webpack.html"&gt;"hot reloading" server&lt;/a&gt; for dev work, and no server for production builds. Running Webpack for production will yield a file &lt;code&gt;article.js&lt;/code&gt; in the root, based on starting at the &lt;code&gt;component/App.jsx&lt;/code&gt; file, and then just bundling in every little requirement that has until we run out of things that need to be bundled in.&lt;/p&gt;
&lt;p&gt;You can see I'm using &lt;a href="http://babeljs.io"&gt;Babel&lt;/a&gt; for interpreting any js/jsx files (it takes care of JSX transforming and ES2015 syntax), as well as &lt;a href="http://lesscss.org"&gt;less&lt;/a&gt; for my CSS. There's also a dumb &lt;a href="https://www.npmjs.com/package/file-loader"&gt;"file-loader"&lt;/a&gt; which simply copies images into their own dir. Not always necessary, but useful when running the webpack dev server.&lt;/p&gt;
&lt;p&gt;The way I trigger either dev or prod runs is via &lt;a href="http://www.slideshare.net/k88hudson/advanced-frontend-automation-with-npm-scripts"&gt;npm scripts&lt;/a&gt;, with the &lt;code&gt;package.json&lt;/code&gt; using the following script block:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  ...
  "scripts": {
    "build": "webpack --prod",
    "start": "webpack-dev-server --progress --colors --hot --inline",
  },
  ...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Two commands, a short &lt;code&gt;$&amp;gt; npm start&lt;/code&gt; for dev work, since that gets run all the time, and a special &lt;code&gt;$&amp;gt; npm run build&lt;/code&gt; for when the production bundle needs to be built. So far so good!&lt;/p&gt;
&lt;h2 id="adding-latex-to-react-components"&gt;Adding LaTeX to React components&lt;/h2&gt;
&lt;p&gt;One thing I hate when doing rewrites is "changing everything" to suit the technology. The article as it exists relies on LaTeX that takes the following form:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;p&amp;gt;\[
  f(x) = \sum^n_{k=0} \frac{n(n-k)}{k! + x} 
\]&amp;lt;/p&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;That's a pretty standard format if you use MathJax, or &lt;a href="https://github.com/Khan/KaTeX"&gt;KaTeX&lt;/a&gt;, and since I use a &lt;em&gt;lot&lt;/em&gt; of LaTeX, that format had to stay. And that's a bit of a problem, because LaTeX uses a lot of curly brackets. And React's JSX syntax treats curly brackets as templating delimiters.&lt;/p&gt;
&lt;p&gt;Something like this, for instance:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;p&amp;gt;\[
  \frac{percentage}{100}
\]&amp;lt;/p&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;will cause the JSX transform step to go "uh, I don't know a variable &lt;code&gt;percentage&lt;/code&gt;, I can't convert this for you". Even though we don't want it converted. Too bad for us! However, there is a way out, and it's called a Webpack loader.&lt;/p&gt;
&lt;h2 id="webpack-and-mathjax-node"&gt;Webpack and mathjax-node&lt;/h2&gt;
&lt;p&gt;You may have noticed the part where I mentioned Webpack uses babel for JSX and ES2015 transforms. That means it rewrites the source code several times before handing it off for bundling, and so if we can convert those LaTeX blocks before anything else tries to interpret it, things should work pretty well!&lt;/p&gt;
&lt;p&gt;So, let us write a latex-loader for Webpack. In fact, I've already done the work there, so, let's look at that. First off, Webpack &lt;strong&gt;is weird&lt;/strong&gt; when it comes to loader order, and uses a &lt;a href="https://en.wikipedia.org/wiki/Stack_%28abstract_data_type%29"&gt;LIFO&lt;/a&gt; ordering. The last loader gets to run first. So:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;loaders: [
  ...
  {
    test: /.jsx?$/,
    include: /components/,
    loaders: [
      'babel-loader',
      __dirname + '/lib/latex-loader'
    ]
  }
]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With a corresponding &lt;code&gt;latex-loader.js&lt;/code&gt; in the &lt;code&gt;./lib&lt;/code&gt; dir:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var op = "\\[";
var ed = "\\]";

/**
 * Is there going to be anything to convert here?
 */
function hasLaTeX(input) {
  return input.indexOf(op) !== -1 &amp;amp;&amp;amp; input.indexOf(ed) !== -1;
}

/**
 * We look for MathJax/KaTeX style data, and convert it to something JSX-safe 
 */
function escapeBlockLaTeX(source) {
  // MAGIC HAPPENS HERE!
  return doTheMagic(source);
}

module.exports = function(source) {
  this.cacheable();
  if (!hasLaTeX(source)) return source;
  return escapeBlockLaTeX(source);
};
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;That's the main gist of it, anyway. The loader's basically a single synchronous function that gets passed entire files as source string, and then expects a possible different string as replacement output.&lt;/p&gt;
&lt;p&gt;So, the approach is obvious: accept the source only if it has LaTeX that needs transforming (otherwise just hand the original source back as a thing we don't need to change), and then if we're still running, find all the LaTeX blocks, transform them to "something safe", and then return that modified source. Ideally, transform them using MathJax.&lt;/p&gt;
&lt;p&gt;As it turns out, MathJax has a server-side library available called &lt;a href="https://www.npmjs.com/package/mathjax-node"&gt;mathjax-node&lt;/a&gt;, which can be used to generate browser-agnostic SVG or MathML source, so that's perfect! It means we can do the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var mathjax = require("mathjax-node");

function escapeBlockLaTeX(source) {
  var from = 0, curr, term, newsource = "", latex;
  for(curr = source.indexOf(op, from);
      curr !== -1;
      from = term + ed.length, curr = source.indexOf(op, from))
  {
    // split out a block of LaTeX:
    newsource += source.substring(from, curr);
    term = source.indexOf(ed, from);
    if(term === -1) {
      // We only have ourselves to blame if we get here
      throw new Error("improperly closed LaTeX encountered!");
    }
    latex = source.substring(curr, term + ed.length);

    // convert this LaTeX code to SVG, which React can deal with just fine:
    var mathjaxed = mathjax.typeset(latex);

    // slot the SVG code in place of the LaTeX code
    newsource += mathjaxed;
  }
  return newsource + source.substring(from);
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Except... we can't.&lt;/p&gt;
&lt;h2 id="webpack-mathjax-and-the-problem-of-sync-vs-async"&gt;Webpack, MathJax, and the problem of sync vs. async&lt;/h2&gt;
&lt;p&gt;You see, Webpack is a fully synchronous technology. A loader gets a file, transforms it, then passes it back to webpack, which hands it to the next loader, which transforms it, and so on. This is a fully synchronous process, and loaders simply get data, and "immediately" (give or take the time needed to modify that data) give something back.&lt;/p&gt;
&lt;p&gt;MathJax can't do that. It relies on quite a few asynchronous things (like font loads), and so where webpack has a function API much like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;module.exports = function webpackloader(input) {
  return formOutput(input);
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;MathJax has a function API like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mathjax.typeset(input, function(result) {
  if(!result.errors) {
    doSomethingWith(result.svg);
  }
});
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;That doesn't look too strange, &lt;a href="http://stackoverflow.com/questions/748175/asynchronous-vs-synchronous-execution-what-does-it-really-mean"&gt;but the timing is crucially different&lt;/a&gt;: when the Webpack loader runs, its function is entered, data is transformed, and it exits again. However, when the MathJax typesetter runs, the function call happens and then &lt;em&gt;immediately exits again&lt;/em&gt;, and at some point in the future the result handling function will run, and there is no way to "wait" for it to finish, because that's not a thing JavaScript can do.&lt;/p&gt;
&lt;p&gt;If only there was a way that we could turn the asynchronouse conversion process that MathJax requires, into a synchronous data transform as is required by Webpack...&lt;/p&gt;
&lt;h2 id="execsync-ing-our-way-to-success"&gt;execSync'ing our way to success&lt;/h2&gt;
&lt;p&gt;And in that wish we find the answer: make the conversion a command line utility, and then &lt;code&gt;exec&lt;/code&gt; that utility synchronously, using &lt;a href="https://nodejs.org"&gt;Node.js&lt;/a&gt;'s built in &lt;a href="https://nodejs.org/api/child_process.html#child_process_child_process_execsync_command_options"&gt;execSync&lt;/a&gt;, because the following will work brilliantly:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var execSync = require("child_process").execSync;

function escapeBlockLaTeX(source) {
  ...
  for(...) {
    // split out a block of LaTeX:
    latex = ...

    // convert all this!
    var mathjaxed = execSync("node tex2svg.js --latex " + latex);

    // slot the rewritten code back in
    newsource += mathjaxed;
  }
  ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We write a simple &lt;code&gt;tex2svg.js&lt;/code&gt; file that requires &lt;code&gt;mathjax-node&lt;/code&gt;, passes it the config options for conversion to SVG, read in the LaTeX as command line argument, and then spits out the conversion result on &lt;a href="https://en.wikipedia.org/wiki/Standard_streams"&gt;standard out&lt;/a&gt;, so that &lt;code&gt;execSync&lt;/code&gt; treats it as its function return value.&lt;/p&gt;
&lt;p&gt;But here's where things get tricky. Writing a command line utility that does the MathJax conversion requires a little work, because the command line is tricky, and LaTeX contains all kinds of characters that can do things in shell land. Slashes and ampersands abound, and those are kind of active symbols,  so we need to be a little smarter:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// convert all this... smarter!
var base64 = new Buffer(latex).toString("base64");
var mathjaxed = execSync("node tex2svg.js --base64 " + base64);
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And then we just need to make sure that the MathJax script unpacks the passed data from &lt;a href="https://en.wikipedia.org/wiki/Base64"&gt;base 64 format&lt;/a&gt; to plain string format before converting, and we're good!&lt;/p&gt;
&lt;p&gt;But not quite: this is not exactly a cheap thing to do. Firing up an instance of node and loading &lt;code&gt;mathjax-node&lt;/code&gt; takes time. Not enough to notice if you only do it once, but if you need to run this lots of times, a second each time adds up to having to wait minutes for this building to happen. Every time you want to run the dev server.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;That is unacceptable.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;So, let's add in some caching: instead of having the MathJax script do the conversion for us, and then spit out the SVG code, we make it do the conversion but then write the SVG code to file, in the images dir.&lt;/p&gt;
&lt;p&gt;And because we don't want the build process to do more work than necessary, we make the filenames predictable, based on the LaTeX that needs to be converted: we compute the &lt;a href="https://en.wikipedia.org/wiki/SHA-1"&gt;sha1 digest&lt;/a&gt; of the LaTeX that needs converting, and only if we don't see a file called &lt;code&gt;&amp;lt;hash&amp;gt;.svg&lt;/code&gt; in the images dir do we do our conversion:  &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var sha1 = require("sha1");

function escapeBlockLaTeX(source) {
  ...
  for(...) {
    latex = ...;

    // Get the digest for the latex - any subsequent checks for the same block
    // of LaTeX will yield the exact same digest, which is handy!
    var hash = sha1.hash(latex);

    // Whether the SVG for this LaTeX already exists or not, we already know
    // the filename it's going to have, so we can prebuild the JSX we need:  
    var imgHTML = '&amp;lt;img src="' + hash + '.svg" className="LaTeX SVG"/&amp;gt;';

    // Then: do we need to generate this image first? If so, execSync,
    // otherwise we do absolutely nothing because we're already done!  
    if (!imageExists(hash)) {
      var base64 = new Buffer(latex).toString("base64");
      execSync("node tex2svg.js --hash " + hash +" --base64 " + base64);
    }

    newsource += imgHTML;
  }
  return newsource;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And that, as they say, is that!&lt;/p&gt;
&lt;h2 id="boom-"&gt;Boom.&lt;/h2&gt;
&lt;p&gt;Using this latex-loader in conjunction with a file writing conversion utility script, we now have a &lt;em&gt;synchronous&lt;/em&gt; MathJax conversion going on, despite MathJax itself being an inherently asynchronous processor, and we completely bypass any strangeness that might normally pop up if you tried to mix LaTeX code and JSX syntax, with the benefit of files that can be cached, so that the browser doesn't need to redownload them everytime the article gets loaded in a browser.&lt;/p&gt;
&lt;p&gt;Responsiveness increase: x1 million. Goal reached.&lt;/p&gt;
&lt;p&gt;Instead of every individual user needing to wait for MathJax to do typesetting, the only person waiting for MathJax now is me, and only for "new" LaTeX blocks during the build step. &lt;em&gt;As it should be&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;I can add sections one at a time, and every time I run the build step, only genuinely new LaTeX will need to get converted. LaTeX that got processed during a previous run already wrote files to disk, so the latex-loader just bypasses the conversion process entirely for those LaTeX blocks, and so the build really only does exactly as much work as it needs to do to get the data transformed.&lt;/p&gt;
&lt;p&gt;Needless to say, I am chuffed to bits about this, and if you have any questions about the process, feel free to reach out to me &lt;a href="https://twitter.com/TheRealPomax"&gt;on twitter&lt;/a&gt; or &lt;a href="https://github.com/Pomax/BezierInfo-2/issues"&gt;on github&lt;/a&gt;, and I'll be happy to talk about it more.&lt;/p&gt;
</description>
<category>React</category>
<category>LaTeX</category>
<category>MathJax</category>
<category>Webpack</category>
<link>http://pomax.github.io/#gh-weblog-1451617530567</link>
<guid>http://pomax.github.io/#gh-weblog-1451617530567</guid>
<pubDate>Fri, 01 Jan 2016 03:05:30 GMT</pubDate>
</item>
<item>
<title> Developing Open Source Software</title>
<description>&lt;p&gt;I want to take a little bit of time to explain how I work on Open Source, both privately and as part of my job as a Software Engineer at the Mozilla Foundation. Not because it's wildly different from how everyone else does it, but because it's probably the same as how the vast majority works on Open Source, which means very few people bother to explain the processes involved.&lt;/p&gt;
&lt;p&gt;There are two different kinds of "working on open source", depending on whether the code is a collaboration or just a simple one-person project, so let's look at both.&lt;/p&gt;
&lt;h2 id="-i-m-making-a-thing-"&gt;&lt;em&gt;"I'm making a thing!"&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;If you're making a thing, the basic rule is "anything goes": you're the only one you're inconveniencing by taking shortcuts, and often that's fine. However, if you're starting a project that you think might at some point gain contributors (say, you're making a thing that you hope becomes popular), there are a few things you can do to make sure that when your project does go from "one dev" to "a team", the transition is smooth:&lt;/p&gt;
&lt;h3 id="file-issues-before-fixing-them"&gt;File issues before fixing them&lt;/h3&gt;
&lt;p&gt;Not only is it a handy If you file the issues you know about as a kind of to-do list to walk through, but you might be surprised to find someone actually fixing an issue you filed before you get to it, once your project gets even a little exposure.&lt;/p&gt;
&lt;h3 id="work-in-branches-"&gt;Work in branches.&lt;/h3&gt;
&lt;p&gt;There will be an initial "I just need to get this code written" period where you're pushing to master: awesome, go for it. However, once you reach what might turn into a 1.0 with a bit more code, start getting in the habit of treating your master branch as off limits, and working in branches that you merge into master instead. This makes it easier for contributors to do the same.&lt;/p&gt;
&lt;h3 id="document-document-document-"&gt;Document, document, document.&lt;/h3&gt;
&lt;p&gt;You're not actually working on your code alone: you're collaborating with your future self, and future self has no idea what you're thinking right now while you're writing your code: document your choices, explain complex bits of code and whatever you do, explain hacks and bodges! Clever as they might be today, 2 months from now they might be so clever you actually need to spend your own time on them to figure out why they even work the way they do. Help future-you out: write documentation.&lt;/p&gt;
&lt;p&gt;And that doesn't need to be wikis or long readmes, it can just be code comments: as long as knowledge you need to understand changes you're making right now isn't lost, you're being awesome. &lt;/p&gt;
&lt;h2 id="-we-re-making-a-thing-"&gt;&lt;em&gt;We're making a thing.&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;While you're fairly free to do what you want to do on your own, for collaborative projects, there is really only one way to work in a way that's not going to break down. For anything that needs to be done, follow the three F's:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;File it&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fix it&lt;/strong&gt;, and&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Follow up&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="1-file-it-"&gt;1) File it!&lt;/h3&gt;
&lt;p&gt;If you're going to work on something, make sure it's a known issue. Code changes without an issue that explain why the changes were necessary in the first place are mystery changes, and mysteries in a collaborative effort are bad.&lt;/p&gt;
&lt;p&gt;However, that doesn't mean the changes aren't necessary, so: always, &lt;strong&gt;always&lt;/strong&gt; make sure there's a ticket, or issue, or bug report, associated with the changes you're making, so that your code changes can say "this fixes/addresses ticket/issue/bug so-and-so". Sometimes those tickets/issues/bug reports already exist and you can simply refer to them, but sometimes they don't: &lt;strong&gt;file it before submitting your code changes&lt;/strong&gt;, so that you can refer to that newly filed ticket.&lt;/p&gt;
&lt;p&gt;It's okay to already have the code in place that addresses an issue you haven't filed yet, just make sure that by the time you submit your changes, there is one.&lt;/p&gt;
&lt;p&gt;Collaboration relies on communication. If people change the code without tying it to the list of "these are issues we need to address", then there is no way to track changes in the codebase. Generating a changelog based on closed issues is often quite easy, but if there are no issues that got closed/resolved due to changes getting accepted into the code base, then you're asking people to work on code that potentially no one can explain (because the person who contributed it may have left already. If you can even track who submitted the change at all).&lt;/p&gt;
&lt;p&gt;Also, &lt;strong&gt;file individual issues&lt;/strong&gt;. The best code bases are ones where each thing that needs to be done is filed separately, and fixed separately, because it's much easier to work on as a team (small tasks make for rapid progress), and it makes it easy to track complex tasks: if you need to implement a user profile system, and that requires a login system, a user database, and user facing UI, then if someone files "implement a user profile system", the very first thing that should happen is chopping that issue up into several smaller issues. It might sound anal, but you're working in a team, and many hands make light work: the smaller you can chop up an issue, the easier it becomes to resolve the bigger task.&lt;/p&gt;
&lt;h3 id="2-fix-it-"&gt;2) Fix it...&lt;/h3&gt;
&lt;p&gt;Crazy as it may sound: never start fixing things by writing code. First ask: "Has someone else already written the code and can I just plug that in?". If they have: just use that. You're still probably going to need a little bit of code to do the "plugging it in" part, but little bits of code are easy to maintain, and it means you're not responsible for maintaining lots of code.&lt;/p&gt;
&lt;p&gt;Conversely, if there is no code out there that already does what you need to do, ask yourself: "Can I write this as a standalone utility, and then plug &lt;em&gt;that&lt;/em&gt; in?". Because if you can, that's worth doing. If you need to solve a problem and there's no solution out there, you're probably solving a problem that other people are also having: it's worth making that solution available.&lt;/p&gt;
&lt;p&gt;Of course, there will be plenty of issues that can only be addressed by writing real, project-relevant, code, and for those occasions there are three things to keep in mind:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;commit early&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;commit often&lt;/strong&gt;, and&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;communicate with your team&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="commit-early"&gt;Commit early&lt;/h4&gt;
&lt;p&gt;If you're working on code changes, push up your changes as soon as you have "something" going. &lt;strong&gt;Especially if it's not done yet&lt;/strong&gt;. Don't wait until "it's done", because you have no idea when that will be: form a commit and push it up once you have the basic stubs in place, for instance, before you start working out the code in full.&lt;/p&gt;
&lt;p&gt;This lets other people that work with you see what you're working on, and lets them catch things early that would cost a lot of time to fix later on. &lt;/p&gt;
&lt;h4 id="commit-often"&gt;Commit often&lt;/h4&gt;
&lt;p&gt;Some code changes are one liners or a simple function renaming, but many are not: don't wait until you're done to push up your changes. Any time you write some code and test it, and it passes, that's a moment to form a commit, push it up, and then keep going.&lt;/p&gt;
&lt;p&gt;If a computer dies (and if you work in a team, that will happen surprisingly often) or someone unexpectedly becomes unavailable for a few days (again, happens more often than you might think), there is no loss of work &lt;strong&gt;for the team&lt;/strong&gt;. The team as a whole can pick up where you as a person left off in these circumstances, and if you don't commit often, they'll potentially have to reinvent changes you had already written.&lt;/p&gt;
&lt;p&gt;It's also much easier to rebase your code if they're small incremental commits when the master code base changes. For instance, a dependency got updated, which caused some functions to use that dependency's new API calls; that is much easier to deal with if it just requires you to change the small commit that touched a file for which that was the case than if you have one massive commit.&lt;/p&gt;
&lt;p&gt;Additionally, the more often you commit, the earlier possible bugs can be found; the earlier bugs are found, the less work it is to fix them, because not a lot of things will trigger them yet.&lt;/p&gt;
&lt;h4 id="communicate-with-your-team"&gt;Communicate with your team&lt;/h4&gt;
&lt;p&gt;If you're working on anything even moderately sized: start talking about your code with team members early. Don't ask them to review only all the way at the end if your changes involve new code or new approaches; run it by someone so that even if you're the only one that'll end up writing code, you're &lt;em&gt;not&lt;/em&gt; the only one who knows what decisions were made while the code was being written.&lt;/p&gt;
&lt;p&gt;Also, remember to ask questions in the open. You might end up with blocking questions that need an answer before you can continue your work, and while it's tempting to try to find someone to get it answered in real-time, &lt;strong&gt;file it first&lt;/strong&gt;, so that the entire team can see it. Then you can find someone to real-time answer it and capture the answer in the filed issue, so that the entire team can be aware of the question having come up, and the answer that was agreed on.&lt;/p&gt;
&lt;h4 id="corrolary-know-when-to-split-your-work-"&gt;Corrolary: know when to split your work.&lt;/h4&gt;
&lt;p&gt;Some issues reveal problems in other parts of the project, and you might be tempted to fix those as part of your changes. I know it's tempting, but &lt;strong&gt;don't&lt;/strong&gt;, because you're not actually helping the team that way. Instead: file it, fix it, and follow up.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Step away from the current code when you reach a good break point (and that could be immediately, if the thing you found is blocking you),&lt;/li&gt;
&lt;li&gt;File the issue as a new issue,&lt;/li&gt;
&lt;li&gt;If it's blocking you:&lt;ol&gt;
&lt;li&gt;Fix it first,&lt;/li&gt;
&lt;li&gt;Schedule follow up&lt;/li&gt;
&lt;li&gt;Rebase your code on the fix, so you're unblocked&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Now you can come back to the code you were already working on.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You'll note that in step 3 the advice is to work on it immediately only if it's actually blocking you: this is important. You're working in a team, and someone else might have a free moment to work on the thing you just discovered, while you keep working on your own changes.&lt;/p&gt;
&lt;h3 id="3-run-through-the-follow-up-"&gt;3) Run through the follow-up.&lt;/h3&gt;
&lt;p&gt;You've worked on code changes, you committed early, and often, and your "patch" now consists of 12 commits and two observations about future work: it's time for follow-up.&lt;/p&gt;
&lt;h4 id="clean-up-your-code"&gt;Clean up your code&lt;/h4&gt;
&lt;p&gt;If your changes work using 12 commits, then your changes work, and it's time to squash those 12 commits into a single commit so it can be landed into the codebase without all the steps that got you there. Even if there is no requirement to squash your code before landing, changelog generation, revision control, and rollbacks are all much nicer if patches land as single commits.&lt;/p&gt;
&lt;p&gt;Also, if there are any unnecessary comments or logs/prints in your code, now's the time to get rid of those, and of course, now is also the time to make sure that any missing documentation either gets added, or gets filed as "document XYZ", to be worked on immediately after landing your changes, rather than anything else, which brings us to...&lt;/p&gt;
&lt;h4 id="file-anything-you-found-but-didn-t-fix-"&gt;File anything you found, but didn't fix.&lt;/h4&gt;
&lt;p&gt;While you were working on your changes, you may have thought of things that might need addressing outside of the changes you made: &lt;strong&gt;file those&lt;/strong&gt;. It is important to capture those observations in a way that the entire team can see them.&lt;/p&gt;
&lt;h4 id="talk-to-your-team"&gt;Talk to your team&lt;/h4&gt;
&lt;p&gt;Finally, follow up with people, too. Let the people who need to know about your changes know about your changes - ask them to review you patch, explain your work to them where needed, if there is testing involved, make sure they understand what needs to be done, and generally make sure at least two people agree these changes are good to go (including yourself). That communication doesn't need to happen in person, the issue tracker you use might facilitate this kind of follow up, but always collaborate on the landing, even if the code works. There might be last minute changes or decisions that you were not aware of that someone else might: good to discover that before the changes are merged in!&lt;/p&gt;
&lt;h2 id="that-s-pretty-much-it"&gt;That's pretty much it&lt;/h2&gt;
&lt;p&gt;There's a fair amount of finer detail and variation that fits into the "file it, fix it, follow up" process, but stick to that order and you're on the path of a sustainable development cycle.&lt;/p&gt;
&lt;p&gt;Most of this is probably obvious to most people, but that just makes it all the more important to get it written down, because someone's going to be a bit bewildered and they'll need a blog post to get them on track =)&lt;/p&gt;
</description>
<category>Open source</category>
<category>Development</category>
<category>Process</category>
<category>Mozilla</category>
<link>http://pomax.github.io/#gh-weblog-1450207129290</link>
<guid>http://pomax.github.io/#gh-weblog-1450207129290</guid>
<pubDate>Tue, 15 Dec 2015 19:18:49 GMT</pubDate>
</item></channel>
</rss>
